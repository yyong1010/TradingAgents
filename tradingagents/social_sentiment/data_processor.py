"""
æ•°æ®å¤„ç†å’Œè¿‡æ»¤æ¨¡å—
è¿‡æ»¤åƒåœ¾ä¿¡æ¯ï¼Œä¿ç•™çœŸå®å®¢æˆ·æ„è§
"""

import re
import jieba
from typing import List, Dict, Set
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class DataProcessor:
    """æ•°æ®å¤„ç†å’Œè¿‡æ»¤å™¨"""
    
    def __init__(self):
        # åƒåœ¾ä¿¡æ¯å…³é”®è¯
        self.spam_keywords = {
            'å¹¿å‘Š', 'æ¨å¹¿', 'åŠ ç¾¤', 'å¾®ä¿¡', 'QQ', 'è”ç³»', 'ä»£ç†', 'æŠ•èµ„é¡¾é—®',
            'èè‚¡', 'å†…å¹•', 'å¿…æ¶¨', 'å¿…è·Œ', 'åŒ…èµš', 'ç¨³èµš', 'æš´æ¶¨', 'æš´è·Œ',
            'è€å¸ˆ', 'ç¾¤ä¸»', 'å¸¦å•', 'è·Ÿå•', 'æ”¶è´¹', 'ä»˜è´¹', 'ä¼šå‘˜', 'VIP',
            'ç‚’è‚¡è½¯ä»¶', 'é€‰è‚¡å™¨', 'è‚¡ç¥¨è½¯ä»¶', 'äº¤æµç¾¤', 'è‚¡å‹ç¾¤'
        }
        
        # æ— æ„ä¹‰å†…å®¹å…³é”®è¯
        self.meaningless_keywords = {
            'é¡¶', 'æ²™å‘', 'æ¿å‡³', 'è·¯è¿‡', 'å›´è§‚', 'åƒç“œ', 'å“ˆå“ˆ', 'å‘µå‘µ',
            'ä¸é”™', 'æ”¯æŒ', 'èµ', 'ğŸ‘', 'ğŸ’ª', 'ğŸš€', 'ğŸ“ˆ', 'ğŸ“‰',
            'æ—©ä¸Šå¥½', 'æ™šä¸Šå¥½', 'ç­¾åˆ°', 'æ‰“å¡'
        }
        
        # æƒ…ç»ªå…³é”®è¯
        self.positive_keywords = {
            'çœ‹å¥½', 'ä¹è§‚', 'ä¸Šæ¶¨', 'æ¶¨', 'ä¹°å…¥', 'åŠ ä»“', 'æŒæœ‰', 'åˆ©å¥½',
            'çªç ´', 'å¼ºåŠ¿', 'ç‰›å¸‚', 'æœºä¼š', 'æ½œåŠ›', 'ä»·å€¼', 'ä½ä¼°',
            'åå¼¹', 'å›å‡', 'ä¼ç¨³', 'æ”¯æ’‘', 'åº•éƒ¨', 'æŠ„åº•'
        }
        
        self.negative_keywords = {
            'çœ‹ç©º', 'æ‚²è§‚', 'ä¸‹è·Œ', 'è·Œ', 'å–å‡º', 'å‡ä»“', 'æ¸…ä»“', 'åˆ©ç©º',
            'ç ´ä½', 'å¼±åŠ¿', 'ç†Šå¸‚', 'é£é™©', 'é«˜ä¼°', 'æ³¡æ²«', 'å¥—ç‰¢',
            'ä¸‹è·Œ', 'æš´è·Œ', 'è·³æ°´', 'å‹åŠ›', 'é¡¶éƒ¨', 'é€ƒé¡¶'
        }
        
        # åˆå§‹åŒ–jiebaåˆ†è¯
        jieba.initialize()
    
    def is_spam(self, text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåƒåœ¾ä¿¡æ¯
        
        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            Trueè¡¨ç¤ºæ˜¯åƒåœ¾ä¿¡æ¯
        """
        if not text or len(text.strip()) < 5:
            return True
            
        text_lower = text.lower()
        
        # æ£€æŸ¥åƒåœ¾å…³é”®è¯
        for keyword in self.spam_keywords:
            if keyword in text_lower:
                return True
                
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šç‰¹æ®Šå­—ç¬¦
        special_char_ratio = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text)) / len(text)
        if special_char_ratio > 0.3:
            return True
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å­—ç¬¦
        if len(set(text)) / len(text) < 0.3:
            return True
            
        return False
    
    def is_meaningless(self, text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ„ä¹‰å†…å®¹
        
        Args:
            text: å¾…æ£€æŸ¥çš„æ–‡æœ¬
            
        Returns:
            Trueè¡¨ç¤ºæ— æ„ä¹‰
        """
        if not text or len(text.strip()) < 3:
            return True
            
        text_clean = text.strip()
        
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«æ— æ„ä¹‰å…³é”®è¯
        words = jieba.lcut(text_clean)
        meaningful_words = [w for w in words if w not in self.meaningless_keywords and len(w) > 1]
        
        if len(meaningful_words) == 0:
            return True
            
        # æ£€æŸ¥æ˜¯å¦è¿‡çŸ­ä¸”æ— å®è´¨å†…å®¹
        if len(text_clean) < 10 and not any(kw in text_clean for kw in self.positive_keywords | self.negative_keywords):
            return True
            
        return False
    
    def extract_sentiment_keywords(self, text: str) -> Dict[str, List[str]]:
        """
        æå–æƒ…ç»ªå…³é”®è¯
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            åŒ…å«æ­£é¢å’Œè´Ÿé¢å…³é”®è¯çš„å­—å…¸
        """
        words = jieba.lcut(text)
        
        positive_found = [w for w in words if w in self.positive_keywords]
        negative_found = [w for w in words if w in self.negative_keywords]
        
        return {
            'positive': positive_found,
            'negative': negative_found
        }
    
    def calculate_sentiment_score(self, text: str) -> float:
        """
        è®¡ç®—æƒ…ç»ªåˆ†æ•°
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            æƒ…ç»ªåˆ†æ•° (-1åˆ°1ä¹‹é—´ï¼Œæ­£æ•°è¡¨ç¤ºæ­£é¢æƒ…ç»ª)
        """
        sentiment_keywords = self.extract_sentiment_keywords(text)
        
        positive_count = len(sentiment_keywords['positive'])
        negative_count = len(sentiment_keywords['negative'])
        
        total_count = positive_count + negative_count
        if total_count == 0:
            return 0.0
            
        # è®¡ç®—æƒ…ç»ªåˆ†æ•°
        score = (positive_count - negative_count) / total_count
        return score
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
            
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', '', text)
        
        return text.strip()
    
    def process_comments(self, comments: List[Dict]) -> List[Dict]:
        """
        å¤„ç†è¯„è®ºåˆ—è¡¨ï¼Œè¿‡æ»¤åƒåœ¾ä¿¡æ¯
        
        Args:
            comments: åŸå§‹è¯„è®ºåˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„è¯„è®ºåˆ—è¡¨
        """
        processed_comments = []
        
        for comment in comments:
            if not isinstance(comment, dict) or 'content' not in comment:
                continue
                
            content = comment.get('content', '')
            if not content:
                continue
                
            # æ¸…ç†æ–‡æœ¬
            cleaned_content = self.clean_text(content)
            
            # è¿‡æ»¤åƒåœ¾ä¿¡æ¯
            if self.is_spam(cleaned_content):
                logger.debug(f"è¿‡æ»¤åƒåœ¾ä¿¡æ¯: {cleaned_content[:50]}...")
                continue
                
            # è¿‡æ»¤æ— æ„ä¹‰å†…å®¹
            if self.is_meaningless(cleaned_content):
                logger.debug(f"è¿‡æ»¤æ— æ„ä¹‰å†…å®¹: {cleaned_content[:50]}...")
                continue
                
            # è®¡ç®—æƒ…ç»ªåˆ†æ•°
            sentiment_score = self.calculate_sentiment_score(cleaned_content)
            sentiment_keywords = self.extract_sentiment_keywords(cleaned_content)
            
            # æ„å»ºå¤„ç†åçš„è¯„è®º
            processed_comment = {
                'content': cleaned_content,
                'original_content': content,
                'sentiment_score': sentiment_score,
                'sentiment_keywords': sentiment_keywords,
                'timestamp': comment.get('timestamp'),
                'author': comment.get('author'),
                'source': comment.get('source'),
                'likes': comment.get('likes', 0),
                'replies': comment.get('replies', 0)
            }
            
            processed_comments.append(processed_comment)
            
        logger.info(f"è¯„è®ºå¤„ç†å®Œæˆ: åŸå§‹{len(comments)}æ¡ -> æœ‰æ•ˆ{len(processed_comments)}æ¡")
        return processed_comments
    
    def aggregate_sentiment(self, comments: List[Dict]) -> Dict:
        """
        èšåˆæƒ…ç»ªåˆ†æç»“æœ
        
        Args:
            comments: å¤„ç†åçš„è¯„è®ºåˆ—è¡¨
            
        Returns:
            èšåˆçš„æƒ…ç»ªåˆ†æç»“æœ
        """
        if not comments:
            return {
                'total_comments': 0,
                'average_sentiment': 0.0,
                'positive_ratio': 0.0,
                'negative_ratio': 0.0,
                'neutral_ratio': 0.0,
                'top_positive_keywords': [],
                'top_negative_keywords': []
            }
            
        # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
        total_comments = len(comments)
        sentiment_scores = [c['sentiment_score'] for c in comments]
        average_sentiment = sum(sentiment_scores) / total_comments
        
        # åˆ†ç±»ç»Ÿè®¡
        positive_count = sum(1 for score in sentiment_scores if score > 0.1)
        negative_count = sum(1 for score in sentiment_scores if score < -0.1)
        neutral_count = total_comments - positive_count - negative_count
        
        positive_ratio = positive_count / total_comments
        negative_ratio = negative_count / total_comments
        neutral_ratio = neutral_count / total_comments
        
        # ç»Ÿè®¡å…³é”®è¯
        all_positive_keywords = []
        all_negative_keywords = []
        
        for comment in comments:
            all_positive_keywords.extend(comment['sentiment_keywords']['positive'])
            all_negative_keywords.extend(comment['sentiment_keywords']['negative'])
            
        # è·å–é«˜é¢‘å…³é”®è¯
        from collections import Counter
        positive_counter = Counter(all_positive_keywords)
        negative_counter = Counter(all_negative_keywords)
        
        top_positive_keywords = positive_counter.most_common(10)
        top_negative_keywords = negative_counter.most_common(10)
        
        return {
            'total_comments': total_comments,
            'average_sentiment': round(average_sentiment, 3),
            'positive_ratio': round(positive_ratio, 3),
            'negative_ratio': round(negative_ratio, 3),
            'neutral_ratio': round(neutral_ratio, 3),
            'top_positive_keywords': top_positive_keywords,
            'top_negative_keywords': top_negative_keywords,
            'sentiment_distribution': {
                'positive': positive_count,
                'negative': negative_count,
                'neutral': neutral_count
            }
        }