"""
æ–°é—»æ•°æ®æºå®ç°
æ”¯æŒä»æ–°æµªè´¢ç»å’Œä¸œæ–¹è´¢å¯Œçˆ¬å–è‚¡ç¥¨æ–°é—»
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import logging
from urllib.parse import urljoin, urlparse

logger = logging.getLogger(__name__)

class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    def __init__(self, title: str, content: str, url: str, publish_time: datetime, source: str):
        self.title = title
        self.content = content
        self.url = url
        self.publish_time = publish_time
        self.source = source
        self.hash_key = self._generate_hash()
    
    def _generate_hash(self) -> str:
        """ç”Ÿæˆæ–°é—»çš„å”¯ä¸€æ ‡è¯†ç”¨äºå»é‡"""
        import hashlib
        content_for_hash = f"{self.title}_{self.publish_time.strftime('%Y%m%d')}"
        return hashlib.md5(content_for_hash.encode('utf-8')).hexdigest()
    
    def is_within_days(self, days: int = 14) -> bool:
        """æ£€æŸ¥æ–°é—»æ˜¯å¦åœ¨æŒ‡å®šå¤©æ•°å†…"""
        cutoff_date = datetime.now() - timedelta(days=days)
        return self.publish_time >= cutoff_date
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'title': self.title,
            'content': self.content,
            'url': self.url,
            'publish_time': self.publish_time.isoformat(),
            'source': self.source,
            'hash_key': self.hash_key
        }

class BaseNewsSource:
    """æ–°é—»æºåŸºç±»"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.timeout = 10
        self.max_retries = 3
    
    def _make_request(self, url: str, **kwargs) -> Optional[requests.Response]:
        """å‘èµ·HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.timeout, **kwargs)
                response.raise_for_status()
                return response
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(1)
        return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        # å¸¸è§çš„æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2})',  # 2024-01-15 10:30
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\s+(\d{2}):(\d{2})',  # 2024å¹´1æœˆ15æ—¥ 10:30
            r'(\d{2})-(\d{2})\s+(\d{2}):(\d{2})',  # 01-15 10:30 (å½“å¹´)
            r'(\d{1,2})æœˆ(\d{1,2})æ—¥\s+(\d{2}):(\d{2})',  # 1æœˆ15æ—¥ 10:30 (å½“å¹´)
            r'(\d{4})/(\d{2})/(\d{2})\s+(\d{2}):(\d{2})',  # 2024/01/15 10:30
        ]
        
        current_year = datetime.now().year
        
        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                try:
                    if len(groups) == 5:
                        if len(groups[0]) == 4:  # åŒ…å«å¹´ä»½
                            year, month, day, hour, minute = map(int, groups)
                        else:  # ä¸åŒ…å«å¹´ä»½ï¼Œä½¿ç”¨å½“å‰å¹´ä»½
                            month, day, hour, minute = map(int, groups)
                            year = current_year
                    elif len(groups) == 4:  # MM-DD HH:MM æ ¼å¼
                        month, day, hour, minute = map(int, groups)
                        year = current_year
                    else:
                        continue
                    
                    return datetime(year, month, day, hour, minute)
                except ValueError:
                    continue
        
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›å½“å‰æ—¶é—´
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸ: {date_str}")
        return datetime.now()
    
    def get_news(self, stock_code: str) -> List[NewsItem]:
        """è·å–æ–°é—»ï¼Œå­ç±»éœ€è¦å®ç°"""
        raise NotImplementedError

class SinaNewsSource(BaseNewsSource):
    """æ–°æµªè´¢ç»æ–°é—»æº"""
    
    def __init__(self):
        super().__init__()
        self.source_name = "æ–°æµªè´¢ç»"
    
    def get_news(self, stock_code: str) -> List[NewsItem]:
        """è·å–æ–°æµªè´¢ç»çš„è‚¡ç¥¨æ–°é—»"""
        news_items = []
        
        # è·å–å…¬å¸æ–°é—»
        corp_news = self._get_corp_news(stock_code)
        news_items.extend(corp_news)
        
        # è·å–ç ”æŠ¥æ–°é—»
        report_news = self._get_report_news(stock_code)
        news_items.extend(report_news)
        
        return news_items
    
    def _get_corp_news(self, stock_code: str) -> List[NewsItem]:
        """è·å–å…¬å¸æ–°é—»"""
        news_items = []
        
        # æ„å»ºURL - æ–°æµªè´¢ç»å…¬å¸æ–°é—»é¡µé¢
        if stock_code.startswith('3'):
            symbol = f"sz{stock_code}"
        elif stock_code.startswith('0'):
            symbol = f"sz{stock_code}"
        elif stock_code.startswith('6'):
            symbol = f"sh{stock_code}"
        else:
            symbol = f"sz{stock_code}"
        
        url = f"https://vip.stock.finance.sina.com.cn/corp/go.php/vCB_AllNewsStock/symbol/{symbol}.phtml"
        
        logger.info(f"ğŸ” [æ–°æµªè´¢ç»] è·å–å…¬å¸æ–°é—»: {url}")
        
        response = self._make_request(url)
        if not response:
            return news_items
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨ - æ›´æ–°çš„é€‰æ‹©å™¨
            news_div = soup.find('div', {'class': 'datelist'})
            if not news_div:
                logger.warning("æœªæ‰¾åˆ°æ–°é—»åˆ—è¡¨div")
                return news_items
            
            news_ul = news_div.find('ul')
            if not news_ul:
                logger.warning("æœªæ‰¾åˆ°æ–°é—»åˆ—è¡¨ul")
                return news_items
            
            # è§£ææ–°é—»åˆ—è¡¨çš„æ–‡æœ¬å†…å®¹
            news_text = news_ul.get_text()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£ææ–°é—»æ¡ç›®
            import re
            # åŒ¹é…æ ¼å¼ï¼šæ—¥æœŸ æ—¶é—´ æ ‡é¢˜é“¾æ¥
            pattern = r'(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2})\s+([^<]+?)(?=\s+\d{4}-\d{2}-\d{2}|\s*$)'
            
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = news_ul.find_all('a', href=True)
            
            for link in links:
                try:
                    title = link.get_text(strip=True)
                    if not title or len(title) < 5:
                        continue
                    
                    news_url = link.get('href', '')
                    if not news_url.startswith('http'):
                        news_url = urljoin(url, news_url)
                    
                    # ä»é“¾æ¥çš„å‰é¢æ–‡æœ¬ä¸­æå–æ—¶é—´
                    link_parent = link.parent
                    if link_parent:
                        parent_text = link_parent.get_text()
                        # æŸ¥æ‰¾æ—¶é—´æ¨¡å¼
                        time_match = re.search(r'(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2})', parent_text)
                        if time_match:
                            date_str = f"{time_match.group(1)} {time_match.group(2)}"
                            publish_time = self._parse_date(date_str)
                        else:
                            publish_time = datetime.now()
                    else:
                        publish_time = datetime.now()
                    
                    # è·å–æ–°é—»å†…å®¹
                    content = self._get_news_content(news_url)
                    if not content:
                        content = title  # å¦‚æœæ— æ³•è·å–å†…å®¹ï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹
                    
                    if title:
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=news_url,
                            publish_time=publish_time,
                            source=f"{self.source_name}-å…¬å¸æ–°é—»"
                        )
                        
                        # åªä¿ç•™2å‘¨å†…çš„æ–°é—»
                        if news_item.is_within_days(14):
                            news_items.append(news_item)
                            logger.info(f"âœ… è·å–æ–°é—»: {title[:50]}...")
                        else:
                            logger.debug(f"â° è·³è¿‡è¿‡æœŸæ–°é—»: {title[:50]}...")
                
                except Exception as e:
                    logger.error(f"è§£ææ–°é—»é“¾æ¥å¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£ææ–°æµªè´¢ç»å…¬å¸æ–°é—»å¤±è´¥: {e}")
        
        return news_items
    
    def _get_report_news(self, stock_code: str) -> List[NewsItem]:
        """è·å–ç ”æŠ¥æ–°é—»"""
        news_items = []
        
        url = f"https://stock.finance.sina.com.cn/stock/go.php/vReport_List/kind/search/index.phtml?symbol={stock_code}&t1=all"
        
        logger.info(f"ğŸ” [æ–°æµªè´¢ç»] è·å–ç ”æŠ¥æ–°é—»: {url}")
        
        response = self._make_request(url)
        if not response:
            return news_items
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾ç ”æŠ¥åˆ—è¡¨
            report_list = soup.find('div', {'class': 'datelist'})
            if not report_list:
                logger.warning("æœªæ‰¾åˆ°ç ”æŠ¥åˆ—è¡¨")
                return news_items
            
            items = report_list.find_all('li')
            
            for item in items:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_link = item.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    news_url = urljoin(url, title_link.get('href', ''))
                    
                    # æå–æ—¶é—´
                    time_span = item.find('span', {'class': 'time'})
                    if time_span:
                        time_str = time_span.get_text(strip=True)
                        publish_time = self._parse_date(time_str)
                    else:
                        publish_time = datetime.now()
                    
                    # è·å–å†…å®¹
                    content = self._get_news_content(news_url)
                    
                    if title and content:
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=news_url,
                            publish_time=publish_time,
                            source=f"{self.source_name}-ç ”æŠ¥"
                        )
                        
                        if news_item.is_within_days(14):
                            news_items.append(news_item)
                            logger.info(f"âœ… è·å–ç ”æŠ¥: {title[:50]}...")
                
                except Exception as e:
                    logger.error(f"è§£æç ”æŠ¥é¡¹å¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£ææ–°æµªè´¢ç»ç ”æŠ¥å¤±è´¥: {e}")
        
        return news_items
    
    def _get_news_content(self, url: str) -> str:
        """è·å–æ–°é—»è¯¦ç»†å†…å®¹"""
        if not url:
            return ""
        
        response = self._make_request(url)
        if not response:
            return ""
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'div.article-content',
                'div.content',
                'div.news-content',
                'div#artibody',
                'div.artibody',
                'div.blkContainerSblk'
            ]
            
            content = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in content_div(["script", "style"]):
                        script.decompose()
                    
                    content = content_div.get_text(strip=True)
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹åŒºåŸŸï¼Œå°è¯•è·å–é¡µé¢ä¸»è¦æ–‡æœ¬
            if not content:
                # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
                    element.decompose()
                
                content = soup.get_text(strip=True)
                # é™åˆ¶å†…å®¹é•¿åº¦
                if len(content) > 2000:
                    content = content[:2000] + "..."
            
            return content[:1500] if content else ""  # é™åˆ¶å†…å®¹é•¿åº¦
        
        except Exception as e:
            logger.error(f"è·å–æ–°é—»å†…å®¹å¤±è´¥: {url} - {e}")
            return ""

class EastmoneyNewsSource(BaseNewsSource):
    """ä¸œæ–¹è´¢å¯Œæ–°é—»æº"""
    
    def __init__(self):
        super().__init__()
        self.source_name = "ä¸œæ–¹è´¢å¯Œ"
    
    def get_news(self, stock_code: str) -> List[NewsItem]:
        """è·å–ä¸œæ–¹è´¢å¯Œçš„è‚¡ç¥¨æ–°é—»"""
        news_items = []
        
        # ä¸œæ–¹è´¢å¯Œè‚¡å§æ–°é—»
        guba_news = self._get_guba_news(stock_code)
        news_items.extend(guba_news)
        
        return news_items
    
    def _get_guba_news(self, stock_code: str) -> List[NewsItem]:
        """è·å–ä¸œæ–¹è´¢å¯Œè‚¡å§æ–°é—»"""
        news_items = []
        
        url = f"https://guba.eastmoney.com/list,{stock_code},1,f.html"
        
        logger.info(f"ğŸ” [ä¸œæ–¹è´¢å¯Œ] è·å–è‚¡å§æ–°é—»: {url}")
        
        response = self._make_request(url)
        if not response:
            return news_items
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨è¡¨æ ¼ - æ›´æ–°çš„é€‰æ‹©å™¨
            news_table = soup.find('table', {'class': 'default_list'})
            if not news_table:
                logger.warning("æœªæ‰¾åˆ°è‚¡å§æ–°é—»åˆ—è¡¨è¡¨æ ¼")
                return news_items
            
            # æŸ¥æ‰¾è¡¨æ ¼ä¸»ä½“
            tbody = news_table.find('tbody', {'class': 'listbody'})
            if not tbody:
                logger.warning("æœªæ‰¾åˆ°è‚¡å§æ–°é—»åˆ—è¡¨ä¸»ä½“")
                return news_items
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»è¡Œ
            rows = tbody.find_all('tr', {'class': 'listitem'})
            
            for row in rows[:20]:  # é™åˆ¶è·å–æ•°é‡
                try:
                    cells = row.find_all('td')
                    if len(cells) < 5:
                        continue
                    
                    # æå–æ ‡é¢˜å’Œé“¾æ¥ï¼ˆç¬¬3åˆ—ï¼‰
                    title_cell = cells[2]
                    title_div = title_cell.find('div', {'class': 'title'})
                    if not title_div:
                        continue
                    
                    title_link = title_div.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    if not title or len(title) < 5:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                        continue
                    
                    post_url = urljoin(url, title_link.get('href', ''))
                    
                    # æå–æ—¶é—´ï¼ˆç¬¬5åˆ—ï¼‰
                    time_cell = cells[4]
                    time_div = time_cell.find('div', {'class': 'update'})
                    if time_div:
                        time_str = time_div.get_text(strip=True)
                        # ä¸œæ–¹è´¢å¯Œçš„æ—¶é—´æ ¼å¼ï¼š07-25 06:39
                        publish_time = self._parse_date(f"2025-{time_str}")
                    else:
                        publish_time = datetime.now()
                    
                    # è·å–å¸–å­å†…å®¹
                    content = self._get_post_content(post_url)
                    if not content:
                        content = title  # å¦‚æœæ— æ³•è·å–å†…å®¹ï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹
                    
                    if title and len(content) > 10:  # ç¡®ä¿æœ‰å®è´¨å†…å®¹
                        news_item = NewsItem(
                            title=title,
                            content=content,
                            url=post_url,
                            publish_time=publish_time,
                            source=f"{self.source_name}-è‚¡å§"
                        )
                        
                        if news_item.is_within_days(14):
                            news_items.append(news_item)
                            logger.info(f"âœ… è·å–è‚¡å§å¸–å­: {title[:50]}...")
                        else:
                            logger.debug(f"â° è·³è¿‡è¿‡æœŸå¸–å­: {title[:50]}...")
                
                except Exception as e:
                    logger.error(f"è§£æè‚¡å§å¸–å­å¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"è§£æä¸œæ–¹è´¢å¯Œè‚¡å§å¤±è´¥: {e}")
        
        return news_items
    
    def _get_post_content(self, url: str) -> str:
        """è·å–å¸–å­è¯¦ç»†å†…å®¹"""
        if not url:
            return ""
        
        response = self._make_request(url)
        if not response:
            return ""
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content_selectors = [
                'div.stockcodec',
                'div.articleZoom',
                'div.content',
                'div.post-content',
                'div#zwconttb'
            ]
            
            content = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in content_div(["script", "style"]):
                        script.decompose()
                    
                    content = content_div.get_text(strip=True)
                    break
            
            return content[:1000] if content else ""  # é™åˆ¶å†…å®¹é•¿åº¦
        
        except Exception as e:
            logger.error(f"è·å–å¸–å­å†…å®¹å¤±è´¥: {url} - {e}")
            return ""