"""
æ–°é—»çˆ¬è™«ä¸»æ§åˆ¶å™¨
è´Ÿè´£åè°ƒå„ä¸ªæ–°é—»æºï¼Œå»é‡å’Œæ•°æ®æ•´ç†
"""

from typing import List, Dict, Set
from datetime import datetime, timedelta
import logging
from .news_sources import NewsItem, SinaNewsSource, EastmoneyNewsSource

logger = logging.getLogger(__name__)

class NewsCrawler:
    """æ–°é—»çˆ¬è™«ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.sources = [
            SinaNewsSource(),
            EastmoneyNewsSource()
        ]
        self.seen_hashes: Set[str] = set()
    
    def get_stock_news(self, stock_code: str, max_days: int = 14) -> Dict:
        """
        è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_days: æœ€å¤§å¤©æ•°ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„æ–°é—»å°†è¢«è¿‡æ»¤
            
        Returns:
            åŒ…å«æ–°é—»æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"ğŸ” [æ–°é—»çˆ¬è™«] å¼€å§‹è·å–è‚¡ç¥¨ {stock_code} çš„æ–°é—»")
        
        all_news = []
        source_stats = {}
        
        # ä»å„ä¸ªæ•°æ®æºè·å–æ–°é—»
        for source in self.sources:
            try:
                logger.info(f"ğŸ“° [æ–°é—»çˆ¬è™«] ä» {source.source_name} è·å–æ–°é—»")
                news_items = source.get_news(stock_code)
                
                # è¿‡æ»¤å’Œå»é‡
                filtered_news = self._filter_and_deduplicate(news_items, max_days)
                
                all_news.extend(filtered_news)
                source_stats[source.source_name] = len(filtered_news)
                
                logger.info(f"âœ… [æ–°é—»çˆ¬è™«] {source.source_name} è·å–åˆ° {len(filtered_news)} æ¡æœ‰æ•ˆæ–°é—»")
                
            except Exception as e:
                logger.error(f"âŒ [æ–°é—»çˆ¬è™«] {source.source_name} è·å–å¤±è´¥: {e}")
                source_stats[source.source_name] = 0
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_news.sort(key=lambda x: x.publish_time, reverse=True)
        
        # é™åˆ¶æ–°é—»æ•°é‡
        max_news = 20
        if len(all_news) > max_news:
            all_news = all_news[:max_news]
            logger.info(f"ğŸ“Š [æ–°é—»çˆ¬è™«] é™åˆ¶æ–°é—»æ•°é‡ä¸º {max_news} æ¡")
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        total_news = len(all_news)
        cutoff_date = datetime.now() - timedelta(days=max_days)
        
        result = {
            'success': total_news > 0,
            'total_news': total_news,
            'cutoff_date': cutoff_date.strftime('%Y-%m-%d'),
            'source_stats': source_stats,
            'news_items': [news.to_dict() for news in all_news],
            'summary': self._generate_summary(all_news, source_stats)
        }
        
        if total_news == 0:
            logger.warning(f"âš ï¸ [æ–°é—»çˆ¬è™«] æœªè·å–åˆ°è‚¡ç¥¨ {stock_code} çš„æœ‰æ•ˆæ–°é—»")
            result['message'] = f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} åœ¨è¿‡å» {max_days} å¤©å†…çš„ç›¸å…³æ–°é—»"
        else:
            logger.info(f"ğŸ‰ [æ–°é—»çˆ¬è™«] æˆåŠŸè·å–è‚¡ç¥¨ {stock_code} çš„ {total_news} æ¡æ–°é—»")
        
        return result
    
    def _filter_and_deduplicate(self, news_items: List[NewsItem], max_days: int) -> List[NewsItem]:
        """è¿‡æ»¤å’Œå»é‡æ–°é—»"""
        filtered_news = []
        
        for news in news_items:
            # æ£€æŸ¥æ—¶é—´èŒƒå›´
            if not news.is_within_days(max_days):
                continue
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if news.hash_key in self.seen_hashes:
                logger.debug(f"ğŸ”„ [å»é‡] è·³è¿‡é‡å¤æ–°é—»: {news.title[:30]}...")
                continue
            
            # æ£€æŸ¥å†…å®¹è´¨é‡
            if len(news.content) < 50:
                logger.debug(f"ğŸ“ [è¿‡æ»¤] è·³è¿‡å†…å®¹è¿‡çŸ­çš„æ–°é—»: {news.title[:30]}...")
                continue
            
            # è¿‡æ»¤åƒåœ¾æ ‡é¢˜
            spam_keywords = ['å¹¿å‘Š', 'æ¨å¹¿', 'å…è´¹', 'åŠ ç¾¤', 'è‚¡ç¥', 'å¿…æ¶¨', 'å†…å¹•']
            if any(keyword in news.title for keyword in spam_keywords):
                logger.debug(f"ğŸš« [è¿‡æ»¤] è·³è¿‡åƒåœ¾æ–°é—»: {news.title[:30]}...")
                continue
            
            self.seen_hashes.add(news.hash_key)
            filtered_news.append(news)
        
        return filtered_news
    
    def _generate_summary(self, news_items: List[NewsItem], source_stats: Dict) -> str:
        """ç”Ÿæˆæ–°é—»æ‘˜è¦"""
        if not news_items:
            return "æœªè·å–åˆ°æœ‰æ•ˆæ–°é—»æ•°æ®"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_count = len(news_items)
        sources = list(source_stats.keys())
        
        # æ—¶é—´èŒƒå›´
        if news_items:
            latest_time = max(news.publish_time for news in news_items)
            earliest_time = min(news.publish_time for news in news_items)
            time_range = f"{earliest_time.strftime('%m-%d')} è‡³ {latest_time.strftime('%m-%d')}"
        else:
            time_range = "æ— "
        
        # ç”Ÿæˆæ‘˜è¦
        summary_parts = [
            f"ğŸ“Š æ–°é—»ç»Ÿè®¡: å…±è·å– {total_count} æ¡æ–°é—»",
            f"ğŸ“… æ—¶é—´èŒƒå›´: {time_range}",
            f"ğŸ“° æ•°æ®æº: {', '.join(sources)}"
        ]
        
        # å„æºç»Ÿè®¡
        source_details = []
        for source, count in source_stats.items():
            if count > 0:
                source_details.append(f"{source}({count}æ¡)")
        
        if source_details:
            summary_parts.append(f"ğŸ“ˆ æ¥æºåˆ†å¸ƒ: {', '.join(source_details)}")
        
        # æœ€æ–°æ–°é—»æ ‡é¢˜
        if news_items:
            latest_news = news_items[0]
            summary_parts.append(f"ğŸ”¥ æœ€æ–°æ–°é—»: {latest_news.title[:50]}...")
        
        return " | ".join(summary_parts)
    
    def get_formatted_news_text(self, stock_code: str, max_days: int = 14) -> str:
        """
        è·å–æ ¼å¼åŒ–çš„æ–°é—»æ–‡æœ¬ï¼Œç”¨äºLLMåˆ†æ
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_days: æœ€å¤§å¤©æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„æ–°é—»æ–‡æœ¬
        """
        news_data = self.get_stock_news(stock_code, max_days)
        
        if not news_data['success']:
            return f"âš ï¸ æ–°é—»è·å–å¤±è´¥: {news_data.get('message', 'æœªçŸ¥é”™è¯¯')}"
        
        # æ„å»ºæ ¼å¼åŒ–æ–‡æœ¬
        text_parts = [
            f"ğŸ“° è‚¡ç¥¨ {stock_code} æ–°é—»åˆ†ææ•°æ®",
            f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {news_data['summary']}",
            "=" * 60,
            ""
        ]
        
        # æ·»åŠ æ–°é—»å†…å®¹
        for i, news_dict in enumerate(news_data['news_items'], 1):
            publish_time = datetime.fromisoformat(news_dict['publish_time'])
            time_str = publish_time.strftime('%m-%d %H:%M')
            
            news_section = [
                f"ğŸ“„ æ–°é—» {i}: {news_dict['title']}",
                f"ğŸ•’ æ—¶é—´: {time_str} | ğŸ“ æ¥æº: {news_dict['source']}",
                f"ğŸ”— é“¾æ¥: {news_dict['url']}",
                f"ğŸ“ å†…å®¹: {news_dict['content'][:800]}{'...' if len(news_dict['content']) > 800 else ''}",
                "-" * 40,
                ""
            ]
            
            text_parts.extend(news_section)
        
        return "\n".join(text_parts)