"""
Analysis History Administration Web Interface

This module provides a web interface for history data management operations,
including cleanup, monitoring, and backup functionality.
"""

import streamlit as st
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from pathlib import Path

from web.utils.history_data_manager import get_data_manager
from web.utils.error_handler import show_error_to_user, show_success_to_user

# Setup logging
logger = logging.getLogger(__name__)


def render_history_admin() -> None:
    """Render the history administration interface"""
    st.title("ðŸ“Š åˆ†æžåŽ†å²ç®¡ç†")
    st.markdown("ç®¡ç†åˆ†æžåŽ†å²æ•°æ®ï¼ŒåŒ…æ‹¬æ¸…ç†ã€ç›‘æŽ§å’Œå¤‡ä»½åŠŸèƒ½")
    
    # Check if data manager is available
    data_manager = get_data_manager()
    if not data_manager.is_available():
        st.error("âŒ æ•°æ®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥MongoDBè¿žæŽ¥")
        return
    
    # Create tabs for different admin functions
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“Š å­˜å‚¨ç»Ÿè®¡", "ðŸ§¹ æ•°æ®æ¸…ç†", "ðŸ’¾ æ•°æ®å¤‡ä»½", "ðŸ” ç›‘æŽ§å‘Šè­¦"])
    
    with tab1:
        render_storage_statistics()
    
    with tab2:
        render_data_cleanup()
    
    with tab3:
        render_data_backup()
    
    with tab4:
        render_monitoring_alerts()


def render_storage_statistics() -> None:
    """Render storage statistics section"""
    st.header("å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯")
    
    data_manager = get_data_manager()
    
    # Add refresh button
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("ðŸ”„ åˆ·æ–°ç»Ÿè®¡", key="refresh_stats"):
            st.rerun()
    
    # Get storage statistics
    with st.spinner("èŽ·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯..."):
        stats = data_manager.get_storage_statistics()
    
    if not stats["success"]:
        st.error(f"èŽ·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return
    
    storage_info = stats["storage_info"]
    
    # Display basic storage metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="æ€»æ–‡æ¡£æ•°",
            value=f"{storage_info['total_documents']:,}",
            help="æ•°æ®åº“ä¸­çš„åˆ†æžè®°å½•æ€»æ•°"
        )
    
    with col2:
        st.metric(
            label="å­˜å‚¨å¤§å°",
            value=f"{storage_info['total_size_mb']:.2f} MB",
            help="æ•°æ®å ç”¨çš„å­˜å‚¨ç©ºé—´"
        )
    
    with col3:
        st.metric(
            label="ç´¢å¼•å¤§å°",
            value=f"{storage_info['index_size_mb']:.2f} MB",
            help="æ•°æ®åº“ç´¢å¼•å ç”¨çš„ç©ºé—´"
        )
    
    with col4:
        avg_size_kb = storage_info['average_document_size_bytes'] / 1024
        st.metric(
            label="å¹³å‡æ–‡æ¡£å¤§å°",
            value=f"{avg_size_kb:.2f} KB",
            help="æ¯ä¸ªåˆ†æžè®°å½•çš„å¹³å‡å¤§å°"
        )
    
    # Status distribution chart
    if stats["status_distribution"]:
        st.subheader("çŠ¶æ€åˆ†å¸ƒ")
        status_data = stats["status_distribution"]
        
        # Create a simple bar chart
        import pandas as pd
        df = pd.DataFrame(list(status_data.items()), columns=['çŠ¶æ€', 'æ•°é‡'])
        st.bar_chart(df.set_index('çŠ¶æ€'))
    
    # Market distribution
    if stats["market_distribution"]:
        st.subheader("å¸‚åœºåˆ†å¸ƒ")
        market_data = stats["market_distribution"]
        
        col1, col2 = st.columns(2)
        with col1:
            for market, count in market_data.items():
                percentage = (count / storage_info['total_documents']) * 100
                st.write(f"**{market}**: {count:,} ({percentage:.1f}%)")
    
    # Performance statistics
    if stats["performance_stats"]:
        perf = stats["performance_stats"]
        st.subheader("æ€§èƒ½ç»Ÿè®¡")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if "avg_execution_time" in perf and perf["avg_execution_time"]:
                st.metric(
                    label="å¹³å‡æ‰§è¡Œæ—¶é—´",
                    value=f"{perf['avg_execution_time']:.2f}ç§’"
                )
                st.metric(
                    label="æœ€é•¿æ‰§è¡Œæ—¶é—´",
                    value=f"{perf.get('max_execution_time', 0):.2f}ç§’"
                )
        
        with col2:
            if "avg_cost" in perf and perf["avg_cost"]:
                st.metric(
                    label="å¹³å‡æˆæœ¬",
                    value=f"${perf['avg_cost']:.4f}"
                )
                st.metric(
                    label="æ€»æˆæœ¬",
                    value=f"${perf.get('total_cost', 0):.2f}"
                )
    
    # Recent activity
    if stats["daily_counts_last_30_days"]:
        st.subheader("æœ€è¿‘æ´»åŠ¨ (è¿‡åŽ»30å¤©)")
        daily_data = stats["daily_counts_last_30_days"]
        
        # Convert to DataFrame for plotting
        import pandas as pd
        df = pd.DataFrame(list(daily_data.items()), columns=['æ—¥æœŸ', 'åˆ†æžæ•°é‡'])
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        df = df.sort_values('æ—¥æœŸ')
        
        st.line_chart(df.set_index('æ—¥æœŸ'))


def render_data_cleanup() -> None:
    """Render data cleanup section"""
    st.header("æ•°æ®æ¸…ç†")
    
    data_manager = get_data_manager()
    
    # Cleanup old records section
    st.subheader("æ¸…ç†æ—§è®°å½•")
    
    col1, col2 = st.columns(2)
    
    with col1:
        max_age_days = st.number_input(
            "ä¿ç•™å¤©æ•°",
            min_value=1,
            max_value=3650,
            value=365,
            help="åˆ é™¤è¶…è¿‡æŒ‡å®šå¤©æ•°çš„è®°å½•"
        )
        
        batch_size = st.number_input(
            "æ‰¹å¤„ç†å¤§å°",
            min_value=10,
            max_value=1000,
            value=100,
            help="æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡"
        )
    
    with col2:
        dry_run = st.checkbox(
            "é¢„è§ˆæ¨¡å¼",
            value=True,
            help="åªæ˜¾ç¤ºå°†è¦åˆ é™¤çš„è®°å½•ï¼Œä¸å®žé™…åˆ é™¤"
        )
    
    if st.button("ðŸ§¹ æ¸…ç†æ—§è®°å½•", key="cleanup_old"):
        with st.spinner("æ­£åœ¨æ¸…ç†æ—§è®°å½•..."):
            result = data_manager.cleanup_old_records(
                max_age_days=max_age_days,
                batch_size=batch_size,
                dry_run=dry_run
            )
        
        if result["success"]:
            if dry_run:
                st.info(f"ðŸ“Š é¢„è§ˆ: å°†åˆ é™¤ {result['total_found']} æ¡è®°å½•")
                if "sample_records" in result and result["sample_records"]:
                    st.write("ç¤ºä¾‹è®°å½•:")
                    for record in result["sample_records"][:5]:
                        st.write(f"- {record['analysis_id']}: {record['stock_symbol']} ({record['created_at']})")
            else:
                if result["deleted_count"] > 0:
                    show_success_to_user(f"æˆåŠŸåˆ é™¤ {result['deleted_count']} æ¡è®°å½•")
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è®°å½•")
        else:
            show_error_to_user(f"æ¸…ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    st.divider()
    
    # Cleanup failed records section
    st.subheader("æ¸…ç†å¤±è´¥è®°å½•")
    
    col1, col2 = st.columns(2)
    
    with col1:
        failed_age_hours = st.number_input(
            "å¤±è´¥è®°å½•ä¿ç•™å°æ—¶æ•°",
            min_value=1,
            max_value=168,  # 1 week
            value=24,
            help="åˆ é™¤è¶…è¿‡æŒ‡å®šå°æ—¶æ•°çš„å¤±è´¥è®°å½•"
        )
    
    with col2:
        failed_dry_run = st.checkbox(
            "é¢„è§ˆæ¨¡å¼",
            value=True,
            key="failed_dry_run",
            help="åªæ˜¾ç¤ºå°†è¦åˆ é™¤çš„å¤±è´¥è®°å½•ï¼Œä¸å®žé™…åˆ é™¤"
        )
    
    if st.button("ðŸ§¹ æ¸…ç†å¤±è´¥è®°å½•", key="cleanup_failed"):
        with st.spinner("æ­£åœ¨æ¸…ç†å¤±è´¥è®°å½•..."):
            result = data_manager.cleanup_failed_records(
                max_age_hours=failed_age_hours,
                dry_run=failed_dry_run
            )
        
        if result["success"]:
            if failed_dry_run:
                st.info(f"ðŸ“Š é¢„è§ˆ: å°†åˆ é™¤ {result['total_found']} æ¡å¤±è´¥è®°å½•")
            else:
                if result["deleted_count"] > 0:
                    show_success_to_user(f"æˆåŠŸåˆ é™¤ {result['deleted_count']} æ¡å¤±è´¥è®°å½•")
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„å¤±è´¥è®°å½•")
        else:
            show_error_to_user(f"æ¸…ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


def render_data_backup() -> None:
    """Render data backup section"""
    st.header("æ•°æ®å¤‡ä»½")
    
    data_manager = get_data_manager()
    
    # Export section
    st.subheader("å¯¼å‡ºæ•°æ®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        export_path = st.text_input(
            "å¯¼å‡ºæ–‡ä»¶è·¯å¾„",
            value=f"data/backups/history_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            help="å¯¼å‡ºæ–‡ä»¶çš„ä¿å­˜è·¯å¾„"
        )
        
        compress_export = st.checkbox(
            "åŽ‹ç¼©æ–‡ä»¶",
            value=True,
            help="ä½¿ç”¨gzipåŽ‹ç¼©å¯¼å‡ºæ–‡ä»¶"
        )
    
    with col2:
        # Date range filters
        st.write("**è¿‡æ»¤æ¡ä»¶ (å¯é€‰)**")
        
        date_from = st.date_input(
            "å¼€å§‹æ—¥æœŸ",
            value=None,
            help="åªå¯¼å‡ºæ­¤æ—¥æœŸä¹‹åŽçš„è®°å½•"
        )
        
        date_to = st.date_input(
            "ç»“æŸæ—¥æœŸ",
            value=None,
            help="åªå¯¼å‡ºæ­¤æ—¥æœŸä¹‹å‰çš„è®°å½•"
        )
        
        status_filter = st.selectbox(
            "çŠ¶æ€è¿‡æ»¤",
            options=["", "completed", "failed", "error", "incomplete"],
            help="åªå¯¼å‡ºæŒ‡å®šçŠ¶æ€çš„è®°å½•"
        )
    
    if st.button("ðŸ“¤ å¯¼å‡ºæ•°æ®", key="export_data"):
        # Build filters
        filters = {}
        if date_from:
            filters["date_from"] = datetime.combine(date_from, datetime.min.time())
        if date_to:
            filters["date_to"] = datetime.combine(date_to, datetime.max.time())
        if status_filter:
            filters["status"] = status_filter
        
        with st.spinner("æ­£åœ¨å¯¼å‡ºæ•°æ®..."):
            result = data_manager.export_data(
                output_path=export_path,
                filters=filters,
                compress=compress_export
            )
        
        if result["success"]:
            show_success_to_user(f"æˆåŠŸå¯¼å‡º {result['exported_count']:,} æ¡è®°å½•")
            st.info(f"æ–‡ä»¶è·¯å¾„: {result['output_path']}")
            st.info(f"æ–‡ä»¶å¤§å°: {result['file_size_mb']:.2f} MB")
        else:
            show_error_to_user(f"å¯¼å‡ºå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    st.divider()
    
    # Import section
    st.subheader("å¯¼å…¥æ•°æ®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        import_path = st.text_input(
            "å¯¼å…¥æ–‡ä»¶è·¯å¾„",
            help="è¦å¯¼å…¥çš„JSONæ–‡ä»¶è·¯å¾„"
        )
        
        skip_existing = st.checkbox(
            "è·³è¿‡å·²å­˜åœ¨è®°å½•",
            value=True,
            help="è·³è¿‡å·²å­˜åœ¨çš„è®°å½•ï¼Œé¿å…é‡å¤"
        )
    
    with col2:
        validate_records = st.checkbox(
            "éªŒè¯è®°å½•",
            value=True,
            help="å¯¼å…¥å‰éªŒè¯è®°å½•æ ¼å¼"
        )
        
        batch_size = st.number_input(
            "æ‰¹å¤„ç†å¤§å°",
            min_value=100,
            max_value=5000,
            value=1000,
            help="æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡"
        )
    
    if st.button("ðŸ“¥ å¯¼å…¥æ•°æ®", key="import_data"):
        if not import_path:
            st.error("è¯·è¾“å…¥å¯¼å…¥æ–‡ä»¶è·¯å¾„")
            return
        
        if not Path(import_path).exists():
            st.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {import_path}")
            return
        
        with st.spinner("æ­£åœ¨å¯¼å…¥æ•°æ®..."):
            result = data_manager.import_data(
                input_path=import_path,
                batch_size=batch_size,
                skip_existing=skip_existing,
                validate_records=validate_records
            )
        
        if result["success"]:
            show_success_to_user(f"å¯¼å…¥å®Œæˆ")
            st.info(f"å¯¼å…¥è®°å½•: {result['imported_count']:,}")
            st.info(f"è·³è¿‡è®°å½•: {result['skipped_count']:,}")
            if result['error_count'] > 0:
                st.warning(f"é”™è¯¯è®°å½•: {result['error_count']:,}")
        else:
            show_error_to_user(f"å¯¼å…¥å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


def render_monitoring_alerts() -> None:
    """Render monitoring and alerts section"""
    st.header("ç›‘æŽ§å‘Šè­¦")
    
    data_manager = get_data_manager()
    
    # Alert thresholds
    st.subheader("å‘Šè­¦é˜ˆå€¼è®¾ç½®")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        max_size_mb = st.number_input(
            "æœ€å¤§å­˜å‚¨å¤§å° (MB)",
            min_value=100,
            max_value=10000,
            value=1000,
            help="å­˜å‚¨å¤§å°è¶…è¿‡æ­¤å€¼æ—¶è§¦å‘å‘Šè­¦"
        )
    
    with col2:
        max_documents = st.number_input(
            "æœ€å¤§æ–‡æ¡£æ•°é‡",
            min_value=1000,
            max_value=1000000,
            value=100000,
            help="æ–‡æ¡£æ•°é‡è¶…è¿‡æ­¤å€¼æ—¶è§¦å‘å‘Šè­¦"
        )
    
    with col3:
        max_daily_growth = st.number_input(
            "æœ€å¤§æ—¥å¢žé•¿é‡",
            min_value=100,
            max_value=10000,
            value=1000,
            help="æ—¥å¢žé•¿é‡è¶…è¿‡æ­¤å€¼æ—¶è§¦å‘å‘Šè­¦"
        )
    
    if st.button("ðŸ” æ£€æŸ¥å‘Šè­¦", key="check_alerts"):
        with st.spinner("æ£€æŸ¥å­˜å‚¨å‘Šè­¦..."):
            alerts = data_manager.check_storage_alerts(
                max_size_mb=max_size_mb,
                max_documents=max_documents,
                max_daily_growth=max_daily_growth
            )
        
        if not alerts["success"]:
            st.error(f"æ£€æŸ¥å‘Šè­¦å¤±è´¥: {alerts.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return
        
        # Display alerts
        if alerts["alert_count"] > 0:
            st.error(f"ðŸš¨ å‘çŽ° {alerts['alert_count']} ä¸ªå‘Šè­¦:")
            for alert in alerts["alerts"]:
                st.error(f"- {alert['message']}")
        
        # Display warnings
        if alerts["warning_count"] > 0:
            st.warning(f"âš ï¸ å‘çŽ° {alerts['warning_count']} ä¸ªè­¦å‘Š:")
            for warning in alerts["warnings"]:
                st.warning(f"- {warning['message']}")
        
        # Display success message if no issues
        if alerts["alert_count"] == 0 and alerts["warning_count"] == 0:
            st.success("âœ… æ‰€æœ‰ç›‘æŽ§æ£€æŸ¥é€šè¿‡ï¼Œæ²¡æœ‰å‘çŽ°é—®é¢˜")
        
        # Display current storage info
        storage_stats = alerts["storage_stats"]
        st.subheader("å½“å‰å­˜å‚¨çŠ¶æ€")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            current_size = storage_stats["total_size_mb"]
            size_percentage = (current_size / max_size_mb) * 100
            st.metric(
                label="å­˜å‚¨ä½¿ç”¨çŽ‡",
                value=f"{size_percentage:.1f}%",
                delta=f"{current_size:.2f} MB / {max_size_mb} MB"
            )
        
        with col2:
            current_docs = storage_stats["total_documents"]
            docs_percentage = (current_docs / max_documents) * 100
            st.metric(
                label="æ–‡æ¡£æ•°é‡ä½¿ç”¨çŽ‡",
                value=f"{docs_percentage:.1f}%",
                delta=f"{current_docs:,} / {max_documents:,}"
            )
        
        with col3:
            # Calculate recent growth if possible
            stats = data_manager.get_storage_statistics()
            if stats["success"] and stats["daily_counts_last_30_days"]:
                daily_counts = stats["daily_counts_last_30_days"]
                recent_days = sorted(daily_counts.keys())[-3:]  # Last 3 days
                if len(recent_days) >= 2:
                    recent_growth = sum(daily_counts[day] for day in recent_days) / len(recent_days)
                    growth_percentage = (recent_growth / max_daily_growth) * 100
                    st.metric(
                        label="æ—¥å¢žé•¿çŽ‡",
                        value=f"{growth_percentage:.1f}%",
                        delta=f"{recent_growth:.0f} / {max_daily_growth}"
                    )
    
    st.divider()
    
    # Automated maintenance settings
    st.subheader("è‡ªåŠ¨ç»´æŠ¤è®¾ç½®")
    
    st.info("""
    ðŸ’¡ **è‡ªåŠ¨ç»´æŠ¤å»ºè®®**
    
    å¯ä»¥è®¾ç½®å®šæ—¶ä»»åŠ¡æ¥è‡ªåŠ¨æ‰§è¡Œç»´æŠ¤æ“ä½œ:
    
    1. **æ¯æ—¥æ¸…ç†**: æ¸…ç†å¤±è´¥çš„åˆ†æžè®°å½•
    2. **æ¯å‘¨å¤‡ä»½**: å¯¼å‡ºé‡è¦æ•°æ®è¿›è¡Œå¤‡ä»½
    3. **æ¯æœˆæ¸…ç†**: æ¸…ç†è¶…è¿‡1å¹´çš„æ—§è®°å½•
    4. **ç›‘æŽ§å‘Šè­¦**: å®šæœŸæ£€æŸ¥å­˜å‚¨ä½¿ç”¨æƒ…å†µ
    
    ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¾ç½®å®šæ—¶ä»»åŠ¡:
    ```bash
    # æ¯æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œæ¸…ç†
    0 2 * * * python scripts/maintenance/history_scheduler.py --cleanup-only
    
    # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹æ‰§è¡Œå¤‡ä»½
    0 3 * * 0 python scripts/maintenance/history_scheduler.py --backup-only
    
    # æ¯å°æ—¶æ£€æŸ¥ç›‘æŽ§å‘Šè­¦
    0 * * * * python scripts/maintenance/history_scheduler.py --monitoring-only
    ```
    """)
    
    # Configuration file editor
    config_path = Path("config/history_maintenance.json")
    if config_path.exists():
        st.subheader("ç»´æŠ¤é…ç½®")
        
        if st.button("ðŸ“ ç¼–è¾‘é…ç½®æ–‡ä»¶", key="edit_config"):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_content = f.read()
                
                edited_config = st.text_area(
                    "é…ç½®æ–‡ä»¶å†…å®¹ (JSONæ ¼å¼)",
                    value=config_content,
                    height=300,
                    help="ç¼–è¾‘è‡ªåŠ¨ç»´æŠ¤é…ç½®"
                )
                
                if st.button("ðŸ’¾ ä¿å­˜é…ç½®", key="save_config"):
                    try:
                        # Validate JSON
                        json.loads(edited_config)
                        
                        # Save configuration
                        with open(config_path, 'w', encoding='utf-8') as f:
                            f.write(edited_config)
                        
                        show_success_to_user("é…ç½®æ–‡ä»¶ä¿å­˜æˆåŠŸ")
                        st.rerun()
                        
                    except json.JSONDecodeError as e:
                        show_error_to_user(f"JSONæ ¼å¼é”™è¯¯: {e}")
                    except Exception as e:
                        show_error_to_user(f"ä¿å­˜å¤±è´¥: {e}")
                        
            except Exception as e:
                show_error_to_user(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")


# Helper function to integrate with main navigation
def should_show_admin_tab() -> bool:
    """Check if admin tab should be shown (can be extended with permission checks)"""
    # For now, always show admin tab
    # In production, you might want to check user permissions here
    return True