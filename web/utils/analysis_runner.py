"""
è‚¡ç¥¨åˆ†ææ‰§è¡Œå·¥å…·
"""

import sys
import os
import uuid
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger, get_logger_manager
logger = get_logger('web')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# ç¡®ä¿ç¯å¢ƒå˜é‡æ­£ç¡®åŠ è½½ - Dockerç¯å¢ƒä¸­ä¸è¦†ç›–ç¯å¢ƒå˜é‡
is_docker = os.getenv("DOCKER_CONTAINER", "false").lower() == "true"
load_dotenv(project_root / ".env", override=not is_docker)

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import setup_web_logging
logger = setup_web_logging()

# Import error handling utilities
from web.utils.error_handler import (
    with_error_handling, with_retry, ErrorHandler, show_error_to_user,
    show_loading_with_progress, ProgressTracker, validate_and_sanitize_input,
    validate_stock_symbol, validate_date_string, validate_positive_integer, validate_non_empty_string
)

# æ·»åŠ é…ç½®ç®¡ç†å™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¸ªåŠŸèƒ½å·²å¯ç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")

def _clean_results_for_storage(results):
    """
    æ¸…ç†åˆ†æç»“æœä¸­ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼Œä»¥ä¾¿å­˜å‚¨åˆ°MongoDB
    
    Args:
        results: åŸå§‹åˆ†æç»“æœ
        
    Returns:
        æ¸…ç†åçš„ç»“æœå­—å…¸
    """
    import copy
    from langchain_core.messages import BaseMessage
    
    def clean_object(obj):
        """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„ä¸å¯åºåˆ—åŒ–å†…å®¹"""
        if isinstance(obj, BaseMessage):
            # å°†LangChainæ¶ˆæ¯å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            return {
                'type': obj.__class__.__name__,
                'content': str(obj.content),
                'id': getattr(obj, 'id', None),
                'additional_kwargs': getattr(obj, 'additional_kwargs', {}),
                'response_metadata': getattr(obj, 'response_metadata', {})
            }
        elif isinstance(obj, dict):
            return {key: clean_object(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [clean_object(item) for item in obj]
        elif hasattr(obj, '__dict__') and not isinstance(obj, (str, int, float, bool, type(None))):
            # å¯¹äºå…¶ä»–å¤æ‚å¯¹è±¡ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            try:
                return str(obj)
            except:
                return f"<{obj.__class__.__name__} object>"
        else:
            return obj
    
    try:
        # æ·±æ‹·è´åŸå§‹ç»“æœä»¥é¿å…ä¿®æ”¹åŸå¯¹è±¡
        cleaned_results = copy.deepcopy(results)
        
        # æ¸…ç†ç»“æœ
        cleaned_results = clean_object(cleaned_results)
        
        logger.debug(f"Successfully cleaned results for storage")
        return cleaned_results
        
    except Exception as e:
        logger.error(f"Error cleaning results for storage: {e}")
        # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„ç»“æœç»“æ„
        return {
            'stock_symbol': results.get('stock_symbol', 'unknown'),
            'analysis_date': results.get('analysis_date', 'unknown'),
            'analysts': results.get('analysts', []),
            'success': results.get('success', False),
            'error': f"Results cleaning failed: {str(e)}"
        }

def translate_analyst_labels(text):
    """å°†åˆ†æå¸ˆçš„è‹±æ–‡æ ‡ç­¾è½¬æ¢ä¸ºä¸­æ–‡"""
    if not text:
        return text

    # åˆ†æå¸ˆæ ‡ç­¾ç¿»è¯‘æ˜ å°„
    translations = {
        'Bull Analyst:': 'çœ‹æ¶¨åˆ†æå¸ˆ:',
        'Bear Analyst:': 'çœ‹è·Œåˆ†æå¸ˆ:',
        'Risky Analyst:': 'æ¿€è¿›é£é™©åˆ†æå¸ˆ:',
        'Safe Analyst:': 'ä¿å®ˆé£é™©åˆ†æå¸ˆ:',
        'Neutral Analyst:': 'ä¸­æ€§é£é™©åˆ†æå¸ˆ:',
        'Research Manager:': 'ç ”ç©¶ç»ç†:',
        'Portfolio Manager:': 'æŠ•èµ„ç»„åˆç»ç†:',
        'Risk Judge:': 'é£é™©ç®¡ç†å§”å‘˜ä¼š:',
        'Trader:': 'äº¤æ˜“å‘˜:'
    }

    # æ›¿æ¢æ‰€æœ‰è‹±æ–‡æ ‡ç­¾
    for english, chinese in translations.items():
        text = text.replace(english, chinese)

    return text


def add_analysis_completion_hook(session_id, hook_function):
    """
    Add a completion hook for analysis
    
    Args:
        session_id: Analysis session ID
        hook_function: Function to call when analysis completes
    """
    # This is a placeholder for future extensibility
    # Currently, history persistence is handled directly in run_stock_analysis
    logger.debug(f"Analysis completion hook registered for session: {session_id}")


def get_analysis_history_metadata(results):
    """
    Extract metadata for history storage from analysis results
    
    Args:
        results: Analysis results dictionary
        
    Returns:
        Dictionary containing metadata for history storage
    """
    metadata = {
        'session_id': results.get('session_id'),
        'analysis_type': 'comprehensive',
        'success': results.get('success', False),
        'has_demo_data': results.get('is_demo', False),
        'demo_reason': results.get('demo_reason'),
        'created_at': datetime.now().isoformat()
    }
    
    # Add LLM configuration
    if 'llm_provider' in results:
        metadata['llm_config'] = {
            'provider': results['llm_provider'],
            'model': results['llm_model']
        }
    
    # Add analysis configuration
    if 'analysts' in results:
        metadata['analysis_config'] = {
            'analysts': results['analysts'],
            'research_depth': results['research_depth']
        }
    
    return metadata

def extract_risk_assessment(state):
    """ä»åˆ†æçŠ¶æ€ä¸­æå–é£é™©è¯„ä¼°æ•°æ®"""
    try:
        risk_debate_state = state.get('risk_debate_state', {})

        if not risk_debate_state:
            return None

        # æå–å„ä¸ªé£é™©åˆ†æå¸ˆçš„è§‚ç‚¹å¹¶è¿›è¡Œä¸­æ–‡åŒ–
        risky_analysis = translate_analyst_labels(risk_debate_state.get('risky_history', ''))
        safe_analysis = translate_analyst_labels(risk_debate_state.get('safe_history', ''))
        neutral_analysis = translate_analyst_labels(risk_debate_state.get('neutral_history', ''))
        judge_decision = translate_analyst_labels(risk_debate_state.get('judge_decision', ''))

        # æ ¼å¼åŒ–é£é™©è¯„ä¼°æŠ¥å‘Š
        risk_assessment = f"""
## âš ï¸ é£é™©è¯„ä¼°æŠ¥å‘Š

### ğŸ”´ æ¿€è¿›é£é™©åˆ†æå¸ˆè§‚ç‚¹
{risky_analysis if risky_analysis else 'æš‚æ— æ¿€è¿›é£é™©åˆ†æ'}

### ğŸŸ¡ ä¸­æ€§é£é™©åˆ†æå¸ˆè§‚ç‚¹
{neutral_analysis if neutral_analysis else 'æš‚æ— ä¸­æ€§é£é™©åˆ†æ'}

### ğŸŸ¢ ä¿å®ˆé£é™©åˆ†æå¸ˆè§‚ç‚¹
{safe_analysis if safe_analysis else 'æš‚æ— ä¿å®ˆé£é™©åˆ†æ'}

### ğŸ›ï¸ é£é™©ç®¡ç†å§”å‘˜ä¼šæœ€ç»ˆå†³è®®
{judge_decision if judge_decision else 'æš‚æ— é£é™©ç®¡ç†å†³è®®'}

---
*é£é™©è¯„ä¼°åŸºäºå¤šè§’åº¦åˆ†æï¼Œè¯·ç»“åˆä¸ªäººé£é™©æ‰¿å—èƒ½åŠ›åšå‡ºæŠ•èµ„å†³ç­–*
        """.strip()

        return risk_assessment

    except Exception as e:
        logger.info(f"æå–é£é™©è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {e}")
        return None

@with_error_handling(context="è‚¡ç¥¨åˆ†ææ‰§è¡Œ", show_user_error=False)
def run_stock_analysis(stock_symbol, analysis_date, analysts, research_depth, llm_provider, llm_model, market_type="ç¾è‚¡", progress_callback=None, session_id=None):
    """æ‰§è¡Œè‚¡ç¥¨åˆ†æï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œè¿›åº¦è·Ÿè¸ª

    Args:
        stock_symbol: è‚¡ç¥¨ä»£ç 
        analysis_date: åˆ†ææ—¥æœŸ
        analysts: åˆ†æå¸ˆåˆ—è¡¨
        research_depth: ç ”ç©¶æ·±åº¦
        llm_provider: LLMæä¾›å•† (dashscope/deepseek/google)
        llm_model: å¤§æ¨¡å‹åç§°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–°UIçŠ¶æ€
    """
    
    # Initialize progress tracker
    progress_tracker = ProgressTracker(total_steps=10)
    
    def update_progress(message, step=None, total_steps=None):
        """æ›´æ–°è¿›åº¦ï¼ŒåŒ…å«é”™è¯¯è·Ÿè¸ª"""
        if step is not None:
            progress_tracker.update(step, message)
        
        if progress_callback:
            progress_callback(message, step, total_steps)
        logger.info(f"[è¿›åº¦] {message}")
    
    # Validate input parameters
    validation_rules = {
        'stock_symbol': validate_stock_symbol,
        'analysis_date': validate_date_string,
        'research_depth': validate_positive_integer,
        'llm_provider': validate_non_empty_string,
        'llm_model': validate_non_empty_string,
        'market_type': validate_non_empty_string
    }
    
    input_data = {
        'stock_symbol': stock_symbol,
        'analysis_date': analysis_date,
        'research_depth': research_depth,
        'llm_provider': llm_provider,
        'llm_model': llm_model,
        'market_type': market_type
    }
    
    is_valid, sanitized_data, validation_errors = validate_and_sanitize_input(input_data, validation_rules)
    
    if not is_valid:
        error_msg = f"è¾“å…¥å‚æ•°éªŒè¯å¤±è´¥: {'; '.join(validation_errors)}"
        logger.error(error_msg)
        user_error = ErrorHandler.create_user_friendly_error(
            ValueError(error_msg), "å‚æ•°éªŒè¯"
        )
        show_error_to_user(user_error)
        return {
            'success': False,
            'error': error_msg,
            'suggestion': "è¯·æ£€æŸ¥è¾“å…¥å‚æ•°çš„æ ¼å¼å’Œæœ‰æ•ˆæ€§",
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date
        }
    
    # Use sanitized data
    stock_symbol = sanitized_data['stock_symbol']
    analysis_date = sanitized_data['analysis_date']
    research_depth = sanitized_data['research_depth']
    llm_provider = sanitized_data['llm_provider']
    llm_model = sanitized_data['llm_model']
    market_type = sanitized_data['market_type']
    
    # Validate analysts list
    if not analysts or not isinstance(analysts, list) or len(analysts) == 0:
        error_msg = "åˆ†æå¸ˆåˆ—è¡¨ä¸èƒ½ä¸ºç©º"
        logger.error(error_msg)
        user_error = ErrorHandler.create_user_friendly_error(
            ValueError(error_msg), "åˆ†æå¸ˆé…ç½®"
        )
        show_error_to_user(user_error)
        return {
            'success': False,
            'error': error_msg,
            'suggestion': "è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†æå¸ˆ",
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date
        }

    # ä½¿ç”¨ä¼ å…¥çš„session_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆæ–°çš„
    if session_id:
        logger.info(f"Using provided session_id: {session_id}")
    else:
        # ç”Ÿæˆä¼šè¯IDç”¨äºTokenè·Ÿè¸ªå’Œæ—¥å¿—å…³è”
        session_id = f"analysis_{uuid.uuid4().hex[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger.info(f"Generated new session_id: {session_id}")
    
    # Initialize history storage and create analysis record
    history_record = None
    try:
        from web.utils.history_storage import get_history_storage
        from web.models.history_models import AnalysisHistoryRecord, AnalysisStatus
        from datetime import datetime as dt
        
        history_storage = get_history_storage()
        
        # Create analysis history record
        history_record = AnalysisHistoryRecord(
            analysis_id=session_id,
            stock_symbol=stock_symbol,
            stock_name=f"è‚¡ç¥¨{stock_symbol}",  # Temporary name, will be updated after data preparation
            market_type=market_type,
            analysis_date=dt.strptime(analysis_date, '%Y-%m-%d') if isinstance(analysis_date, str) else analysis_date,
            status=AnalysisStatus.IN_PROGRESS.value,
            analysis_type="comprehensive",
            analysts_used=analysts,
            research_depth=research_depth,
            llm_provider=llm_provider,
            llm_model=llm_model,
            metadata={
                'session_id': session_id,
                'user_agent': 'streamlit_web',
                'version': '0.1.2'
            }
        )
        
        logger.info(f"Created analysis history record: {session_id}")
        
    except Exception as e:
        logger.warning(f"Failed to initialize history storage: {e}")
        # Continue without history storage - graceful fallback

    # 1. æ•°æ®é¢„è·å–å’ŒéªŒè¯é˜¶æ®µ
    update_progress("ğŸ” éªŒè¯è‚¡ç¥¨ä»£ç å¹¶é¢„è·å–æ•°æ®...", 1, 10)

    @with_retry(max_attempts=3, delay=2.0, retry_on=(ConnectionError, TimeoutError))
    def prepare_stock_data_with_retry():
        from tradingagents.utils.stock_validator import prepare_stock_data
        return prepare_stock_data(
            stock_code=stock_symbol,
            market_type=market_type,
            period_days=30,  # å¯ä»¥æ ¹æ®research_depthè°ƒæ•´
            analysis_date=analysis_date
        )

    try:
        # é¢„è·å–è‚¡ç¥¨æ•°æ®ï¼ˆé»˜è®¤30å¤©å†å²æ•°æ®ï¼‰
        preparation_result = prepare_stock_data_with_retry()

        if not preparation_result.is_valid:
            error_msg = f"âŒ è‚¡ç¥¨æ•°æ®éªŒè¯å¤±è´¥: {preparation_result.error_message}"
            update_progress(error_msg, 1)
            logger.error(f"[{session_id}] {error_msg}")

            # Create user-friendly error
            user_error = ErrorHandler.create_user_friendly_error(
                ValueError(preparation_result.error_message), "è‚¡ç¥¨æ•°æ®éªŒè¯"
            )
            
            # Save failed validation to history
            if history_record and history_storage.is_available():
                try:
                    history_record.update_status(AnalysisStatus.FAILED.value)
                    history_record.add_metadata('error', preparation_result.error_message)
                    history_record.add_metadata('error_type', 'data_validation_failed')
                    history_record.add_metadata('suggestion', preparation_result.suggestion)
                    history_record.add_metadata('success', False)
                    
                    if history_storage.save_analysis(history_record):
                        logger.info(f"Saved failed validation to history: {session_id}")
                        
                except Exception as history_error:
                    logger.error(f"Error saving failed validation to history: {history_error}")

            return {
                'success': False,
                'error': preparation_result.error_message,
                'suggestion': preparation_result.suggestion,
                'stock_symbol': stock_symbol,
                'analysis_date': analysis_date,
                'session_id': session_id,
                'user_error': user_error
            }

        # æ•°æ®é¢„è·å–æˆåŠŸ
        success_msg = f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {preparation_result.stock_name} ({preparation_result.market_type})"
        update_progress(success_msg, 2)
        logger.info(f"[{session_id}] {success_msg}")
        logger.info(f"[{session_id}] ç¼“å­˜çŠ¶æ€: {preparation_result.cache_status}")
        
        # Update history record with stock name
        if history_record:
            history_record.stock_name = preparation_result.stock_name
            history_record.market_type = preparation_result.market_type

    except Exception as e:
        error_msg = f"âŒ æ•°æ®é¢„è·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        update_progress(error_msg, 1)
        logger.error(f"[{session_id}] {error_msg}")

        # Create user-friendly error
        user_error = ErrorHandler.create_user_friendly_error(e, "æ•°æ®é¢„è·å–")

        # Save data preparation failure to history
        if history_record and history_storage.is_available():
            try:
                history_record.update_status(AnalysisStatus.FAILED.value)
                history_record.add_metadata('error', str(e))
                history_record.add_metadata('error_type', 'data_preparation_failed')
                history_record.add_metadata('success', False)
                
                if history_storage.save_analysis(history_record):
                    logger.info(f"Saved data preparation failure to history: {session_id}")
                    
            except Exception as history_error:
                logger.error(f"Error saving data preparation failure to history: {history_error}")

        return {
            'success': False,
            'error': error_msg,
            'suggestion': "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•",
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date,
            'session_id': session_id,
            'user_error': user_error
        }

    # è®°å½•åˆ†æå¼€å§‹çš„è¯¦ç»†æ—¥å¿—
    logger_manager = get_logger_manager()
    import time
    analysis_start_time = time.time()

    logger_manager.log_analysis_start(
        logger, stock_symbol, "comprehensive_analysis", session_id
    )

    logger.info(f"ğŸš€ [åˆ†æå¼€å§‹] è‚¡ç¥¨åˆ†æå¯åŠ¨",
               extra={
                   'stock_symbol': stock_symbol,
                   'analysis_date': analysis_date,
                   'analysts': analysts,
                   'research_depth': research_depth,
                   'llm_provider': llm_provider,
                   'llm_model': llm_model,
                   'market_type': market_type,
                   'session_id': session_id,
                   'event_type': 'web_analysis_start'
               })

    update_progress("ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ...")

    # ä¼°ç®—Tokenä½¿ç”¨ï¼ˆç”¨äºæˆæœ¬é¢„ä¼°ï¼‰
    if TOKEN_TRACKING_ENABLED:
        estimated_input = 2000 * len(analysts)  # ä¼°ç®—æ¯ä¸ªåˆ†æå¸ˆ2000ä¸ªè¾“å…¥token
        estimated_output = 1000 * len(analysts)  # ä¼°ç®—æ¯ä¸ªåˆ†æå¸ˆ1000ä¸ªè¾“å‡ºtoken
        estimated_cost = token_tracker.estimate_cost(llm_provider, llm_model, estimated_input, estimated_output)

        update_progress(f"ğŸ’° é¢„ä¼°åˆ†ææˆæœ¬: Â¥{estimated_cost:.4f}")

    # éªŒè¯ç¯å¢ƒå˜é‡
    update_progress("æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®...")
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    dashscope_key = os.getenv("DASHSCOPE_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    finnhub_key = os.getenv("FINNHUB_API_KEY")

    logger.info(f"ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    logger.info(f"  DEEPSEEK_API_KEY: {'å·²è®¾ç½®' if deepseek_key else 'æœªè®¾ç½®'}")
    logger.info(f"  DASHSCOPE_API_KEY: {'å·²è®¾ç½®' if dashscope_key else 'æœªè®¾ç½®'}")
    logger.info(f"  OPENAI_API_KEY: {'å·²è®¾ç½®' if openai_key else 'æœªè®¾ç½®'}")
    logger.info(f"  FINNHUB_API_KEY: {'å·²è®¾ç½®' if finnhub_key else 'æœªè®¾ç½®'}")

    # æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªAIæ¨¡å‹APIå¯†é’¥
    if not any([deepseek_key, dashscope_key, openai_key]):
        raise ValueError("è¯·è‡³å°‘é…ç½®ä¸€ä¸ªAIæ¨¡å‹APIå¯†é’¥ï¼šDEEPSEEK_API_KEYã€DASHSCOPE_API_KEY æˆ– OPENAI_API_KEY")
    if not finnhub_key:
        raise ValueError("FINNHUB_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

    update_progress("ç¯å¢ƒå˜é‡éªŒè¯é€šè¿‡")

    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from tradingagents.graph.trading_graph import TradingAgentsGraph
        from tradingagents.default_config import DEFAULT_CONFIG

        # åˆ›å»ºé…ç½®
        update_progress("é…ç½®åˆ†æå‚æ•°...")
        config = DEFAULT_CONFIG.copy()
        config["llm_provider"] = llm_provider
        config["deep_think_llm"] = llm_model
        config["quick_think_llm"] = llm_model
        # æ ¹æ®ç ”ç©¶æ·±åº¦è°ƒæ•´é…ç½®
        if research_depth == 1:  # 1çº§ - å¿«é€Ÿåˆ†æ
            config["max_debate_rounds"] = 1
            config["max_risk_discuss_rounds"] = 1
            # ä¿æŒå†…å­˜åŠŸèƒ½å¯ç”¨ï¼Œå› ä¸ºå†…å­˜æ“ä½œå¼€é”€å¾ˆå°ä½†èƒ½æ˜¾è‘—æå‡åˆ†æè´¨é‡
            config["memory_enabled"] = True

            # ç»Ÿä¸€ä½¿ç”¨åœ¨çº¿å·¥å…·ï¼Œé¿å…ç¦»çº¿å·¥å…·çš„å„ç§é—®é¢˜
            config["online_tools"] = True  # æ‰€æœ‰å¸‚åœºéƒ½ä½¿ç”¨ç»Ÿä¸€å·¥å…·
            logger.info(f"ğŸ”§ [å¿«é€Ÿåˆ†æ] {market_type}ä½¿ç”¨ç»Ÿä¸€å·¥å…·ï¼Œç¡®ä¿æ•°æ®æºæ­£ç¡®å’Œç¨³å®šæ€§")
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-turbo"  # ä½¿ç”¨æœ€å¿«æ¨¡å‹
                config["deep_think_llm"] = "qwen-plus"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"  # DeepSeekåªæœ‰ä¸€ä¸ªæ¨¡å‹
                config["deep_think_llm"] = "deepseek-chat"
        elif research_depth == 2:  # 2çº§ - åŸºç¡€åˆ†æ
            config["max_debate_rounds"] = 1
            config["max_risk_discuss_rounds"] = 1
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-plus"
                config["deep_think_llm"] = "qwen-plus"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"
        elif research_depth == 3:  # 3çº§ - æ ‡å‡†åˆ†æ (é»˜è®¤)
            config["max_debate_rounds"] = 1
            config["max_risk_discuss_rounds"] = 2
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-plus"
                config["deep_think_llm"] = "qwen-max"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"
        elif research_depth == 4:  # 4çº§ - æ·±åº¦åˆ†æ
            config["max_debate_rounds"] = 2
            config["max_risk_discuss_rounds"] = 2
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-plus"
                config["deep_think_llm"] = "qwen-max"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"
        else:  # 5çº§ - å…¨é¢åˆ†æ
            config["max_debate_rounds"] = 3
            config["max_risk_discuss_rounds"] = 3
            config["memory_enabled"] = True
            config["online_tools"] = True
            if llm_provider == "dashscope":
                config["quick_think_llm"] = "qwen-max"
                config["deep_think_llm"] = "qwen-max"
            elif llm_provider == "deepseek":
                config["quick_think_llm"] = "deepseek-chat"
                config["deep_think_llm"] = "deepseek-chat"

        # æ ¹æ®LLMæä¾›å•†è®¾ç½®ä¸åŒçš„é…ç½®
        if llm_provider == "dashscope":
            config["backend_url"] = "https://dashscope.aliyuncs.com/api/v1"
        elif llm_provider == "deepseek":
            config["backend_url"] = "https://api.deepseek.com"
        elif llm_provider == "google":
            # Google AIä¸éœ€è¦backend_urlï¼Œä½¿ç”¨é»˜è®¤çš„OpenAIæ ¼å¼
            config["backend_url"] = "https://api.openai.com/v1"

        # ä¿®å¤è·¯å¾„é—®é¢˜
        config["data_dir"] = str(project_root / "data")
        config["results_dir"] = str(project_root / "results")
        config["data_cache_dir"] = str(project_root / "tradingagents" / "dataflows" / "data_cache")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        update_progress("ğŸ“ åˆ›å»ºå¿…è¦çš„ç›®å½•...")
        os.makedirs(config["data_dir"], exist_ok=True)
        os.makedirs(config["results_dir"], exist_ok=True)
        os.makedirs(config["data_cache_dir"], exist_ok=True)

        logger.info(f"ä½¿ç”¨é…ç½®: {config}")
        logger.info(f"åˆ†æå¸ˆåˆ—è¡¨: {analysts}")
        logger.info(f"è‚¡ç¥¨ä»£ç : {stock_symbol}")
        logger.info(f"åˆ†ææ—¥æœŸ: {analysis_date}")

        # æ ¹æ®å¸‚åœºç±»å‹è°ƒæ•´è‚¡ç¥¨ä»£ç æ ¼å¼
        logger.debug(f"ğŸ” [RUNNER DEBUG] ===== è‚¡ç¥¨ä»£ç æ ¼å¼åŒ– =====")
        logger.debug(f"ğŸ” [RUNNER DEBUG] åŸå§‹è‚¡ç¥¨ä»£ç : '{stock_symbol}'")
        logger.debug(f"ğŸ” [RUNNER DEBUG] å¸‚åœºç±»å‹: '{market_type}'")

        if market_type == "Aè‚¡":
            # Aè‚¡ä»£ç ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¿æŒåŸæ ·
            formatted_symbol = stock_symbol
            logger.debug(f"ğŸ” [RUNNER DEBUG] Aè‚¡ä»£ç ä¿æŒåŸæ ·: '{formatted_symbol}'")
            update_progress(f"ğŸ‡¨ğŸ‡³ å‡†å¤‡åˆ†æAè‚¡: {formatted_symbol}")
        elif market_type == "æ¸¯è‚¡":
            # æ¸¯è‚¡ä»£ç è½¬ä¸ºå¤§å†™ï¼Œç¡®ä¿.HKåç¼€
            formatted_symbol = stock_symbol.upper()
            if not formatted_symbol.endswith('.HK'):
                # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ .HKåç¼€
                if formatted_symbol.isdigit():
                    formatted_symbol = f"{formatted_symbol.zfill(4)}.HK"
            update_progress(f"ğŸ‡­ğŸ‡° å‡†å¤‡åˆ†ææ¸¯è‚¡: {formatted_symbol}")
        else:
            # ç¾è‚¡ä»£ç è½¬ä¸ºå¤§å†™
            formatted_symbol = stock_symbol.upper()
            logger.debug(f"ğŸ” [RUNNER DEBUG] ç¾è‚¡ä»£ç è½¬å¤§å†™: '{stock_symbol}' -> '{formatted_symbol}'")
            update_progress(f"ğŸ‡ºğŸ‡¸ å‡†å¤‡åˆ†æç¾è‚¡: {formatted_symbol}")

        logger.debug(f"ğŸ” [RUNNER DEBUG] æœ€ç»ˆä¼ é€’ç»™åˆ†æå¼•æ“çš„è‚¡ç¥¨ä»£ç : '{formatted_symbol}'")

        # åˆå§‹åŒ–äº¤æ˜“å›¾
        update_progress("ğŸ”§ åˆå§‹åŒ–åˆ†æå¼•æ“...")
        graph = TradingAgentsGraph(analysts, config=config, debug=False)

        # æ‰§è¡Œåˆ†æ
        update_progress(f"ğŸ“Š å¼€å§‹åˆ†æ {formatted_symbol} è‚¡ç¥¨ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")
        logger.debug(f"ğŸ” [RUNNER DEBUG] ===== è°ƒç”¨graph.propagate =====")
        logger.debug(f"ğŸ” [RUNNER DEBUG] ä¼ é€’ç»™graph.propagateçš„å‚æ•°:")
        logger.debug(f"ğŸ” [RUNNER DEBUG]   symbol: '{formatted_symbol}'")
        logger.debug(f"ğŸ” [RUNNER DEBUG]   date: '{analysis_date}'")

        state, decision = graph.propagate(formatted_symbol, analysis_date)

        # è°ƒè¯•ä¿¡æ¯
        logger.debug(f"ğŸ” [DEBUG] åˆ†æå®Œæˆï¼Œdecisionç±»å‹: {type(decision)}")
        logger.debug(f"ğŸ” [DEBUG] decisionå†…å®¹: {decision}")

        # æ ¼å¼åŒ–ç»“æœ
        update_progress("ğŸ“‹ åˆ†æå®Œæˆï¼Œæ­£åœ¨æ•´ç†ç»“æœ...")

        # æå–é£é™©è¯„ä¼°æ•°æ®
        risk_assessment = extract_risk_assessment(state)

        # å°†é£é™©è¯„ä¼°æ·»åŠ åˆ°çŠ¶æ€ä¸­
        if risk_assessment:
            state['risk_assessment'] = risk_assessment

        # è®°å½•Tokenä½¿ç”¨ï¼ˆå®é™…ä½¿ç”¨é‡ï¼Œè¿™é‡Œä½¿ç”¨ä¼°ç®—å€¼ï¼‰
        if TOKEN_TRACKING_ENABLED:
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›å€¼åº”è¯¥ä»LLMå“åº”ä¸­è·å–
            # è¿™é‡Œä½¿ç”¨åŸºäºåˆ†æå¸ˆæ•°é‡å’Œç ”ç©¶æ·±åº¦çš„ä¼°ç®—
            actual_input_tokens = len(analysts) * (1500 if research_depth == "å¿«é€Ÿ" else 2500 if research_depth == "æ ‡å‡†" else 4000)
            actual_output_tokens = len(analysts) * (800 if research_depth == "å¿«é€Ÿ" else 1200 if research_depth == "æ ‡å‡†" else 2000)

            usage_record = token_tracker.track_usage(
                provider=llm_provider,
                model_name=llm_model,
                input_tokens=actual_input_tokens,
                output_tokens=actual_output_tokens,
                session_id=session_id,
                analysis_type=f"{market_type}_analysis"
            )

            if usage_record:
                update_progress(f"ğŸ’° è®°å½•ä½¿ç”¨æˆæœ¬: Â¥{usage_record.cost:.4f}")

        results = {
            'stock_symbol': stock_symbol,
            'analysis_date': analysis_date,
            'analysts': analysts,
            'research_depth': research_depth,
            'llm_provider': llm_provider,
            'llm_model': llm_model,
            'state': state,
            'decision': decision,
            'success': True,
            'error': None,
            'session_id': session_id if TOKEN_TRACKING_ENABLED else None
        }

        # è®°å½•åˆ†æå®Œæˆçš„è¯¦ç»†æ—¥å¿—
        analysis_duration = time.time() - analysis_start_time

        # è®¡ç®—æ€»æˆæœ¬ï¼ˆå¦‚æœæœ‰Tokenè·Ÿè¸ªï¼‰
        total_cost = 0.0
        if TOKEN_TRACKING_ENABLED:
            try:
                total_cost = token_tracker.get_session_cost(session_id)
            except:
                pass

        # Save analysis results to history
        if history_record and history_storage.is_available():
            try:
                # Clean results for MongoDB serialization
                cleaned_results = _clean_results_for_storage(results)
                
                # Generate formatted results for consistent reporting
                formatted_results = format_analysis_results(results)
                
                # Update history record with results and completion data
                history_record.set_execution_time(analysis_duration)
                history_record.add_results(
                    raw_results=cleaned_results,
                    formatted_results=formatted_results
                )
                
                # Add token usage if available
                if TOKEN_TRACKING_ENABLED and total_cost > 0:
                    # Estimate token usage based on cost and provider
                    estimated_input = actual_input_tokens if 'actual_input_tokens' in locals() else len(analysts) * 2000
                    estimated_output = actual_output_tokens if 'actual_output_tokens' in locals() else len(analysts) * 1000
                    history_record.add_token_usage(estimated_input, estimated_output, total_cost)
                
                # Add session metadata
                history_record.add_metadata('analysis_duration', analysis_duration)
                history_record.add_metadata('success', True)
                
                # Update status to completed
                history_record.update_status(AnalysisStatus.COMPLETED.value)
                
                # Save to database
                if history_storage.save_analysis(history_record):
                    logger.info(f"Successfully saved analysis to history: {session_id}")
                    update_progress("ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°å†å²è®°å½•")
                else:
                    logger.warning(f"Failed to save analysis to history: {session_id}")
                    
            except Exception as e:
                logger.error(f"Error saving analysis to history: {e}")
                # Continue without failing the analysis

        logger_manager.log_analysis_complete(
            logger, stock_symbol, "comprehensive_analysis", session_id,
            analysis_duration, total_cost
        )

        logger.info(f"âœ… [åˆ†æå®Œæˆ] è‚¡ç¥¨åˆ†ææˆåŠŸå®Œæˆ",
                   extra={
                       'stock_symbol': stock_symbol,
                       'session_id': session_id,
                       'duration': analysis_duration,
                       'total_cost': total_cost,
                       'analysts_used': analysts,
                       'success': True,
                       'event_type': 'web_analysis_complete'
                   })

        update_progress("âœ… åˆ†ææˆåŠŸå®Œæˆï¼")
        return results

    except Exception as e:
        # è®°å½•åˆ†æå¤±è´¥çš„è¯¦ç»†æ—¥å¿—
        analysis_duration = time.time() - analysis_start_time

        # Save failed analysis to history
        if history_record and history_storage.is_available():
            try:
                history_record.update_status(AnalysisStatus.FAILED.value)
                history_record.set_execution_time(analysis_duration)
                history_record.add_metadata('error', str(e))
                history_record.add_metadata('error_type', type(e).__name__)
                history_record.add_metadata('success', False)
                
                if history_storage.save_analysis(history_record):
                    logger.info(f"Saved failed analysis to history: {session_id}")
                    
            except Exception as history_error:
                logger.error(f"Error saving failed analysis to history: {history_error}")

        logger_manager.log_module_error(
            logger, "comprehensive_analysis", stock_symbol, session_id,
            analysis_duration, str(e)
        )

        logger.error(f"âŒ [åˆ†æå¤±è´¥] è‚¡ç¥¨åˆ†ææ‰§è¡Œå¤±è´¥",
                    extra={
                        'stock_symbol': stock_symbol,
                        'session_id': session_id,
                        'duration': analysis_duration,
                        'error': str(e),
                        'error_type': type(e).__name__,
                        'analysts_used': analysts,
                        'success': False,
                        'event_type': 'web_analysis_error'
                    }, exc_info=True)

        # å¦‚æœçœŸå®åˆ†æå¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
        return generate_demo_results(stock_symbol, analysis_date, analysts, research_depth, llm_provider, llm_model, str(e), market_type)

def format_analysis_results(results):
    """æ ¼å¼åŒ–åˆ†æç»“æœç”¨äºæ˜¾ç¤º"""
    
    if not results['success']:
        return {
            'error': results['error'],
            'success': False
        }
    
    # Try to update history record with formatted results
    session_id = results.get('session_id')
    if session_id:
        try:
            from web.utils.history_storage import get_history_storage
            history_storage = get_history_storage()
            
            if history_storage.is_available():
                # Get the existing record
                history_record = history_storage.get_analysis_by_id(session_id)
                if history_record:
                    # We'll update the formatted results after processing
                    pass
        except Exception as e:
            logger.warning(f"Could not retrieve history record for formatting: {e}")
    
    state = results['state']
    decision = results['decision']

    # æå–å…³é”®ä¿¡æ¯
    # decision å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "BUY", "SELL", "HOLD"ï¼‰æˆ–å­—å…¸
    if isinstance(decision, str):
        # å°†è‹±æ–‡æŠ•èµ„å»ºè®®è½¬æ¢ä¸ºä¸­æ–‡
        action_translation = {
            'BUY': 'ä¹°å…¥',
            'SELL': 'å–å‡º',
            'HOLD': 'æŒæœ‰',
            'buy': 'ä¹°å…¥',
            'sell': 'å–å‡º',
            'hold': 'æŒæœ‰'
        }
        action = action_translation.get(decision.strip(), decision.strip())

        formatted_decision = {
            'action': action,
            'confidence': 0.7,  # é»˜è®¤ç½®ä¿¡åº¦
            'risk_score': 0.3,  # é»˜è®¤é£é™©åˆ†æ•°
            'target_price': None,  # å­—ç¬¦ä¸²æ ¼å¼æ²¡æœ‰ç›®æ ‡ä»·æ ¼
            'reasoning': f'åŸºäºAIåˆ†æï¼Œå»ºè®®{decision.strip().upper()}'
        }
    elif isinstance(decision, dict):
        # å¤„ç†ç›®æ ‡ä»·æ ¼ - ç¡®ä¿æ­£ç¡®æå–æ•°å€¼
        target_price = decision.get('target_price')
        if target_price is not None and target_price != 'N/A':
            try:
                # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                if isinstance(target_price, str):
                    # ç§»é™¤è´§å¸ç¬¦å·å’Œç©ºæ ¼
                    clean_price = target_price.replace('$', '').replace('Â¥', '').replace('ï¿¥', '').strip()
                    target_price = float(clean_price) if clean_price and clean_price != 'None' else None
                elif isinstance(target_price, (int, float)):
                    target_price = float(target_price)
                else:
                    target_price = None
            except (ValueError, TypeError):
                target_price = None
        else:
            target_price = None

        # å°†è‹±æ–‡æŠ•èµ„å»ºè®®è½¬æ¢ä¸ºä¸­æ–‡
        action_translation = {
            'BUY': 'ä¹°å…¥',
            'SELL': 'å–å‡º',
            'HOLD': 'æŒæœ‰',
            'buy': 'ä¹°å…¥',
            'sell': 'å–å‡º',
            'hold': 'æŒæœ‰'
        }
        action = decision.get('action', 'æŒæœ‰')
        chinese_action = action_translation.get(action, action)

        formatted_decision = {
            'action': chinese_action,
            'confidence': decision.get('confidence', 0.5),
            'risk_score': decision.get('risk_score', 0.3),
            'target_price': target_price,
            'reasoning': decision.get('reasoning', 'æš‚æ— åˆ†ææ¨ç†')
        }
    else:
        # å¤„ç†å…¶ä»–ç±»å‹
        formatted_decision = {
            'action': 'æŒæœ‰',
            'confidence': 0.5,
            'risk_score': 0.3,
            'target_price': None,
            'reasoning': f'åˆ†æç»“æœ: {str(decision)}'
        }
    
    # æ ¼å¼åŒ–çŠ¶æ€ä¿¡æ¯
    formatted_state = {}
    
    # å¤„ç†å„ä¸ªåˆ†ææ¨¡å—çš„ç»“æœ
    analysis_keys = [
        'market_report',
        'fundamentals_report', 
        'sentiment_report',
        'news_report',
        'risk_assessment',
        'investment_plan'
    ]
    
    for key in analysis_keys:
        if key in state:
            # å¯¹æ–‡æœ¬å†…å®¹è¿›è¡Œä¸­æ–‡åŒ–å¤„ç†
            content = state[key]
            if isinstance(content, str):
                content = translate_analyst_labels(content)
            formatted_state[key] = content
    
    formatted_results = {
        'stock_symbol': results['stock_symbol'],
        'decision': formatted_decision,
        'state': formatted_state,
        'success': True,
        # å°†é…ç½®ä¿¡æ¯æ”¾åœ¨é¡¶å±‚ï¼Œä¾›å‰ç«¯ç›´æ¥è®¿é—®
        'analysis_date': results['analysis_date'],
        'analysts': results['analysts'],
        'research_depth': results['research_depth'],
        'llm_provider': results.get('llm_provider', 'dashscope'),
        'llm_model': results['llm_model'],
        'session_id': results.get('session_id'),
        'metadata': {
            'analysis_date': results['analysis_date'],
            'analysts': results['analysts'],
            'research_depth': results['research_depth'],
            'llm_provider': results.get('llm_provider', 'dashscope'),
            'llm_model': results['llm_model'],
            'session_id': results.get('session_id'),
            'formatted_at': datetime.now().isoformat()
        }
    }
    
    # Note: History record update is now handled in the main analysis flow
    # to ensure formatted results are saved consistently with raw results
    
    return formatted_results

def validate_analysis_params(stock_symbol, analysis_date, analysts, research_depth, market_type="ç¾è‚¡"):
    """éªŒè¯åˆ†æå‚æ•°"""

    errors = []

    # éªŒè¯è‚¡ç¥¨ä»£ç 
    if not stock_symbol or len(stock_symbol.strip()) == 0:
        errors.append("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
    elif len(stock_symbol.strip()) > 10:
        errors.append("è‚¡ç¥¨ä»£ç é•¿åº¦ä¸èƒ½è¶…è¿‡10ä¸ªå­—ç¬¦")
    else:
        # æ ¹æ®å¸‚åœºç±»å‹éªŒè¯ä»£ç æ ¼å¼
        symbol = stock_symbol.strip()
        if market_type == "Aè‚¡":
            # Aè‚¡ï¼š6ä½æ•°å­—
            import re
            if not re.match(r'^\d{6}$', symbol):
                errors.append("Aè‚¡ä»£ç æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º6ä½æ•°å­—ï¼ˆå¦‚ï¼š000001ï¼‰")
        elif market_type == "æ¸¯è‚¡":
            # æ¸¯è‚¡ï¼š4-5ä½æ•°å­—.HK æˆ– çº¯4-5ä½æ•°å­—
            import re
            symbol_upper = symbol.upper()
            # æ£€æŸ¥æ˜¯å¦ä¸º XXXX.HK æˆ– XXXXX.HK æ ¼å¼
            hk_format = re.match(r'^\d{4,5}\.HK$', symbol_upper)
            # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯4-5ä½æ•°å­—æ ¼å¼
            digit_format = re.match(r'^\d{4,5}$', symbol)

            if not (hk_format or digit_format):
                errors.append("æ¸¯è‚¡ä»£ç æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º4ä½æ•°å­—.HKï¼ˆå¦‚ï¼š0700.HKï¼‰æˆ–4ä½æ•°å­—ï¼ˆå¦‚ï¼š0700ï¼‰")
        elif market_type == "ç¾è‚¡":
            # ç¾è‚¡ï¼š1-5ä½å­—æ¯
            import re
            if not re.match(r'^[A-Z]{1,5}$', symbol.upper()):
                errors.append("ç¾è‚¡ä»£ç æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º1-5ä½å­—æ¯ï¼ˆå¦‚ï¼šAAPLï¼‰")
    
    # éªŒè¯åˆ†æå¸ˆåˆ—è¡¨
    if not analysts or len(analysts) == 0:
        errors.append("å¿…é¡»è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†æå¸ˆ")
    
    valid_analysts = ['market', 'social', 'news', 'fundamentals']
    invalid_analysts = [a for a in analysts if a not in valid_analysts]
    if invalid_analysts:
        errors.append(f"æ— æ•ˆçš„åˆ†æå¸ˆç±»å‹: {', '.join(invalid_analysts)}")
    
    # éªŒè¯ç ”ç©¶æ·±åº¦
    if not isinstance(research_depth, int) or research_depth < 1 or research_depth > 5:
        errors.append("ç ”ç©¶æ·±åº¦å¿…é¡»æ˜¯1-5ä¹‹é—´çš„æ•´æ•°")
    
    # éªŒè¯åˆ†ææ—¥æœŸ
    try:
        from datetime import datetime
        datetime.strptime(analysis_date, '%Y-%m-%d')
    except ValueError:
        errors.append("åˆ†ææ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œåº”ä¸ºYYYY-MM-DDæ ¼å¼")
    
    return len(errors) == 0, errors

def get_supported_stocks():
    """è·å–æ”¯æŒçš„è‚¡ç¥¨åˆ—è¡¨"""
    
    # å¸¸è§çš„ç¾è‚¡è‚¡ç¥¨ä»£ç 
    popular_stocks = [
        {'symbol': 'AAPL', 'name': 'è‹¹æœå…¬å¸', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'MSFT', 'name': 'å¾®è½¯', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'GOOGL', 'name': 'è°·æ­Œ', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'AMZN', 'name': 'äºšé©¬é€Š', 'sector': 'æ¶ˆè´¹'},
        {'symbol': 'TSLA', 'name': 'ç‰¹æ–¯æ‹‰', 'sector': 'æ±½è½¦'},
        {'symbol': 'NVDA', 'name': 'è‹±ä¼Ÿè¾¾', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'META', 'name': 'Meta', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'NFLX', 'name': 'å¥ˆé£', 'sector': 'åª’ä½“'},
        {'symbol': 'AMD', 'name': 'AMD', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'INTC', 'name': 'è‹±ç‰¹å°”', 'sector': 'ç§‘æŠ€'},
        {'symbol': 'SPY', 'name': 'S&P 500 ETF', 'sector': 'ETF'},
        {'symbol': 'QQQ', 'name': 'çº³æ–¯è¾¾å…‹100 ETF', 'sector': 'ETF'},
    ]
    
    return popular_stocks

def generate_demo_results(stock_symbol, analysis_date, analysts, research_depth, llm_provider, llm_model, error_msg, market_type="ç¾è‚¡"):
    """ç”Ÿæˆæ¼”ç¤ºåˆ†æç»“æœ"""

    import random

    # æ ¹æ®å¸‚åœºç±»å‹è®¾ç½®è´§å¸ç¬¦å·å’Œä»·æ ¼èŒƒå›´
    if market_type == "æ¸¯è‚¡":
        currency_symbol = "HK$"
        price_range = (50, 500)  # æ¸¯è‚¡ä»·æ ¼èŒƒå›´
        market_name = "æ¸¯è‚¡"
    elif market_type == "Aè‚¡":
        currency_symbol = "Â¥"
        price_range = (5, 100)   # Aè‚¡ä»·æ ¼èŒƒå›´
        market_name = "Aè‚¡"
    else:  # ç¾è‚¡
        currency_symbol = "$"
        price_range = (50, 300)  # ç¾è‚¡ä»·æ ¼èŒƒå›´
        market_name = "ç¾è‚¡"

    # ç”Ÿæˆæ¨¡æ‹Ÿå†³ç­–
    actions = ['ä¹°å…¥', 'æŒæœ‰', 'å–å‡º']
    action = random.choice(actions)

    demo_decision = {
        'action': action,
        'confidence': round(random.uniform(0.6, 0.9), 2),
        'risk_score': round(random.uniform(0.2, 0.7), 2),
        'target_price': round(random.uniform(*price_range), 2),
        'reasoning': f"""
åŸºäºå¯¹{market_name}{stock_symbol}çš„ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬çš„AIåˆ†æå›¢é˜Ÿå¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

**æŠ•èµ„å»ºè®®**: {action}
**ç›®æ ‡ä»·æ ¼**: {currency_symbol}{round(random.uniform(*price_range), 2)}

**ä¸»è¦åˆ†æè¦ç‚¹**:
1. **æŠ€æœ¯é¢åˆ†æ**: å½“å‰ä»·æ ¼è¶‹åŠ¿æ˜¾ç¤º{'ä¸Šæ¶¨' if action == 'ä¹°å…¥' else 'ä¸‹è·Œ' if action == 'å–å‡º' else 'æ¨ªç›˜'}ä¿¡å·
2. **åŸºæœ¬é¢è¯„ä¼°**: å…¬å¸è´¢åŠ¡çŠ¶å†µ{'è‰¯å¥½' if action == 'ä¹°å…¥' else 'ä¸€èˆ¬' if action == 'æŒæœ‰' else 'éœ€å…³æ³¨'}
3. **å¸‚åœºæƒ…ç»ª**: æŠ•èµ„è€…æƒ…ç»ª{'ä¹è§‚' if action == 'ä¹°å…¥' else 'ä¸­æ€§' if action == 'æŒæœ‰' else 'è°¨æ…'}
4. **é£é™©è¯„ä¼°**: å½“å‰é£é™©æ°´å¹³ä¸º{'ä¸­ç­‰' if action == 'æŒæœ‰' else 'è¾ƒä½' if action == 'ä¹°å…¥' else 'è¾ƒé«˜'}

**æ³¨æ„**: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®æ­£ç¡®çš„APIå¯†é’¥ã€‚
        """
    }

    # ç”Ÿæˆæ¨¡æ‹ŸçŠ¶æ€æ•°æ®
    demo_state = {}

    if 'market' in analysts:
        current_price = round(random.uniform(*price_range), 2)
        high_price = round(current_price * random.uniform(1.2, 1.8), 2)
        low_price = round(current_price * random.uniform(0.5, 0.8), 2)

        demo_state['market_report'] = f"""
## ğŸ“ˆ {market_name}{stock_symbol} æŠ€æœ¯é¢åˆ†ææŠ¥å‘Š

### ä»·æ ¼è¶‹åŠ¿åˆ†æ
- **å½“å‰ä»·æ ¼**: {currency_symbol}{current_price}
- **æ—¥å†…å˜åŒ–**: {random.choice(['+', '-'])}{round(random.uniform(0.5, 5), 2)}%
- **52å‘¨é«˜ç‚¹**: {currency_symbol}{high_price}
- **52å‘¨ä½ç‚¹**: {currency_symbol}{low_price}

### æŠ€æœ¯æŒ‡æ ‡
- **RSI (14æ—¥)**: {round(random.uniform(30, 70), 1)}
- **MACD**: {'çœ‹æ¶¨' if action == 'BUY' else 'çœ‹è·Œ' if action == 'SELL' else 'ä¸­æ€§'}
- **ç§»åŠ¨å¹³å‡çº¿**: ä»·æ ¼{'é«˜äº' if action == 'BUY' else 'ä½äº' if action == 'SELL' else 'æ¥è¿‘'}20æ—¥å‡çº¿

### æ”¯æ’‘é˜»åŠ›ä½
- **æ”¯æ’‘ä½**: ${round(random.uniform(80, 120), 2)}
- **é˜»åŠ›ä½**: ${round(random.uniform(250, 350), 2)}

*æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®APIå¯†é’¥*
        """

    if 'fundamentals' in analysts:
        demo_state['fundamentals_report'] = f"""
## ğŸ’° {stock_symbol} åŸºæœ¬é¢åˆ†ææŠ¥å‘Š

### è´¢åŠ¡æŒ‡æ ‡
- **å¸‚ç›ˆç‡ (P/E)**: {round(random.uniform(15, 35), 1)}
- **å¸‚å‡€ç‡ (P/B)**: {round(random.uniform(1, 5), 1)}
- **å‡€èµ„äº§æ”¶ç›Šç‡ (ROE)**: {round(random.uniform(10, 25), 1)}%
- **æ¯›åˆ©ç‡**: {round(random.uniform(20, 60), 1)}%

### ç›ˆåˆ©èƒ½åŠ›
- **è¥æ”¶å¢é•¿**: {random.choice(['+', '-'])}{round(random.uniform(5, 20), 1)}%
- **å‡€åˆ©æ¶¦å¢é•¿**: {random.choice(['+', '-'])}{round(random.uniform(10, 30), 1)}%
- **æ¯è‚¡æ”¶ç›Š**: ${round(random.uniform(2, 15), 2)}

### è´¢åŠ¡å¥åº·åº¦
- **è´Ÿå€ºç‡**: {round(random.uniform(20, 60), 1)}%
- **æµåŠ¨æ¯”ç‡**: {round(random.uniform(1, 3), 1)}
- **ç°é‡‘æµ**: {'æ­£å‘' if action != 'SELL' else 'éœ€å…³æ³¨'}

*æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®APIå¯†é’¥*
        """

    if 'social' in analysts:
        demo_state['sentiment_report'] = f"""
## ğŸ’­ {stock_symbol} å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š

### ç¤¾äº¤åª’ä½“æƒ…ç»ª
- **æ•´ä½“æƒ…ç»ª**: {'ç§¯æ' if action == 'BUY' else 'æ¶ˆæ' if action == 'SELL' else 'ä¸­æ€§'}
- **æƒ…ç»ªå¼ºåº¦**: {round(random.uniform(0.5, 0.9), 2)}
- **è®¨è®ºçƒ­åº¦**: {'é«˜' if random.random() > 0.5 else 'ä¸­ç­‰'}

### æŠ•èµ„è€…æƒ…ç»ªæŒ‡æ ‡
- **ææ…Œè´ªå©ªæŒ‡æ•°**: {round(random.uniform(20, 80), 0)}
- **çœ‹æ¶¨çœ‹è·Œæ¯”**: {round(random.uniform(0.8, 1.5), 2)}
- **æœŸæƒPut/Callæ¯”**: {round(random.uniform(0.5, 1.2), 2)}

### æœºæ„æŠ•èµ„è€…åŠ¨å‘
- **æœºæ„æŒä»“å˜åŒ–**: {random.choice(['å¢æŒ', 'å‡æŒ', 'ç»´æŒ'])}
- **åˆ†æå¸ˆè¯„çº§**: {'ä¹°å…¥' if action == 'BUY' else 'å–å‡º' if action == 'SELL' else 'æŒæœ‰'}

*æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®APIå¯†é’¥*
        """

    if 'news' in analysts:
        demo_state['news_report'] = f"""
## ğŸ“° {stock_symbol} æ–°é—»äº‹ä»¶åˆ†ææŠ¥å‘Š

### è¿‘æœŸé‡è¦æ–°é—»
1. **è´¢æŠ¥å‘å¸ƒ**: å…¬å¸å‘å¸ƒ{'è¶…é¢„æœŸ' if action == 'BUY' else 'ä½äºé¢„æœŸ' if action == 'SELL' else 'ç¬¦åˆé¢„æœŸ'}çš„å­£åº¦è´¢æŠ¥
2. **è¡Œä¸šåŠ¨æ€**: æ‰€åœ¨è¡Œä¸šé¢ä¸´{'åˆ©å¥½' if action == 'BUY' else 'æŒ‘æˆ˜' if action == 'SELL' else 'ç¨³å®š'}æ”¿ç­–ç¯å¢ƒ
3. **å…¬å¸å…¬å‘Š**: ç®¡ç†å±‚{'ä¹è§‚' if action == 'BUY' else 'è°¨æ…' if action == 'SELL' else 'ç¨³å¥'}å±•æœ›æœªæ¥

### æ–°é—»æƒ…ç»ªåˆ†æ
- **æ­£é¢æ–°é—»å æ¯”**: {round(random.uniform(40, 80), 0)}%
- **è´Ÿé¢æ–°é—»å æ¯”**: {round(random.uniform(10, 40), 0)}%
- **ä¸­æ€§æ–°é—»å æ¯”**: {round(random.uniform(20, 50), 0)}%

### å¸‚åœºå½±å“è¯„ä¼°
- **çŸ­æœŸå½±å“**: {'æ­£é¢' if action == 'BUY' else 'è´Ÿé¢' if action == 'SELL' else 'ä¸­æ€§'}
- **é•¿æœŸå½±å“**: {'ç§¯æ' if action != 'SELL' else 'éœ€è§‚å¯Ÿ'}

*æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®APIå¯†é’¥*
        """

    # æ·»åŠ é£é™©è¯„ä¼°å’ŒæŠ•èµ„å»ºè®®
    demo_state['risk_assessment'] = f"""
## âš ï¸ {stock_symbol} é£é™©è¯„ä¼°æŠ¥å‘Š

### ä¸»è¦é£é™©å› ç´ 
1. **å¸‚åœºé£é™©**: {'ä½' if action == 'BUY' else 'é«˜' if action == 'SELL' else 'ä¸­ç­‰'}
2. **è¡Œä¸šé£é™©**: {'å¯æ§' if action != 'SELL' else 'éœ€å…³æ³¨'}
3. **å…¬å¸ç‰¹å®šé£é™©**: {'è¾ƒä½' if action == 'BUY' else 'ä¸­ç­‰'}

### é£é™©ç­‰çº§è¯„ä¼°
- **æ€»ä½“é£é™©ç­‰çº§**: {'ä½é£é™©' if action == 'BUY' else 'é«˜é£é™©' if action == 'SELL' else 'ä¸­ç­‰é£é™©'}
- **å»ºè®®ä»“ä½**: {random.choice(['è½»ä»“', 'æ ‡å‡†ä»“ä½', 'é‡ä»“']) if action != 'SELL' else 'å»ºè®®å‡ä»“'}

*æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®APIå¯†é’¥*
    """

    demo_state['investment_plan'] = f"""
## ğŸ“‹ {stock_symbol} æŠ•èµ„å»ºè®®

### å…·ä½“æ“ä½œå»ºè®®
- **æ“ä½œæ–¹å‘**: {action}
- **å»ºè®®ä»·ä½**: ${round(random.uniform(90, 310), 2)}
- **æ­¢æŸä½**: ${round(random.uniform(80, 200), 2)}
- **ç›®æ ‡ä»·ä½**: ${round(random.uniform(150, 400), 2)}

### æŠ•èµ„ç­–ç•¥
- **æŠ•èµ„æœŸé™**: {'çŸ­æœŸ' if research_depth <= 2 else 'ä¸­é•¿æœŸ'}
- **ä»“ä½ç®¡ç†**: {'åˆ†æ‰¹å»ºä»“' if action == 'BUY' else 'åˆ†æ‰¹å‡ä»“' if action == 'SELL' else 'ç»´æŒç°çŠ¶'}

*æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…åˆ†æéœ€è¦é…ç½®APIå¯†é’¥*
    """

    return {
        'stock_symbol': stock_symbol,
        'analysis_date': analysis_date,
        'analysts': analysts,
        'research_depth': research_depth,
        'llm_provider': llm_provider,
        'llm_model': llm_model,
        'state': demo_state,
        'decision': demo_decision,
        'success': True,
        'error': None,
        'is_demo': True,
        'demo_reason': f"APIè°ƒç”¨å¤±è´¥ï¼Œæ˜¾ç¤ºæ¼”ç¤ºæ•°æ®ã€‚é”™è¯¯ä¿¡æ¯: {error_msg}"
    }
