"""
ä¾§è¾¹æ ç»„ä»¶
"""

import streamlit as st
import os

def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ é…ç½®"""

    with st.sidebar:
        # AIæ¨¡å‹é…ç½®
        st.markdown("### ğŸ§  AIæ¨¡å‹é…ç½®")

        # LLMæä¾›å•†é€‰æ‹©
        llm_provider = st.selectbox(
            "LLMæä¾›å•†",
            options=["deepseek", "dashscope", "google", "openai"],
            index=0,  # é»˜è®¤é€‰æ‹©DeepSeek
            format_func=lambda x: {
                "deepseek": "DeepSeek V3",
                "dashscope": "é˜¿é‡Œç™¾ç‚¼",
                "google": "Google AI",
                "openai": "OpenAI"
            }[x],
            help="é€‰æ‹©AIæ¨¡å‹æä¾›å•†"
        )

        # æ ¹æ®æä¾›å•†æ˜¾ç¤ºä¸åŒçš„æ¨¡å‹é€‰é¡¹
        if llm_provider == "deepseek":
            llm_model = st.selectbox(
                "é€‰æ‹©DeepSeekæ¨¡å‹",
                options=["deepseek-chat"],
                index=0,
                format_func=lambda x: {
                    "deepseek-chat": "DeepSeek Chat - é€šç”¨å¯¹è¯æ¨¡å‹ï¼Œé€‚åˆè‚¡ç¥¨åˆ†æ"
                }[x],
                help="é€‰æ‹©ç”¨äºåˆ†æçš„DeepSeekæ¨¡å‹"
            )
        elif llm_provider == "dashscope":
            llm_model = st.selectbox(
                "æ¨¡å‹ç‰ˆæœ¬",
                options=["qwen-turbo", "qwen-plus-latest", "qwen-max"],
                index=1,
                format_func=lambda x: {
                    "qwen-turbo": "Turbo - å¿«é€Ÿ",
                    "qwen-plus-latest": "Plus - å¹³è¡¡",
                    "qwen-max": "Max - æœ€å¼º"
                }[x],
                help="é€‰æ‹©ç”¨äºåˆ†æçš„é˜¿é‡Œç™¾ç‚¼æ¨¡å‹"
            )
        elif llm_provider == "openai":
            llm_model = st.selectbox(
                "é€‰æ‹©OpenAIæ¨¡å‹",
                options=["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"],
                index=1,
                format_func=lambda x: {
                    "gpt-4o": "GPT-4o - æœ€å¼ºæ€§èƒ½",
                    "gpt-4o-mini": "GPT-4o Mini - å¹³è¡¡æ€§ä»·æ¯”",
                    "gpt-3.5-turbo": "GPT-3.5 Turbo - å¿«é€Ÿå“åº”"
                }[x],
                help="é€‰æ‹©ç”¨äºåˆ†æçš„OpenAIæ¨¡å‹"
            )
        else:  # google
            llm_model = st.selectbox(
                "é€‰æ‹©Googleæ¨¡å‹",
                options=["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"],
                index=0,
                format_func=lambda x: {
                    "gemini-2.0-flash": "Gemini 2.0 Flash - æ¨èä½¿ç”¨",
                    "gemini-1.5-pro": "Gemini 1.5 Pro - å¼ºå¤§æ€§èƒ½",
                    "gemini-1.5-flash": "Gemini 1.5 Flash - å¿«é€Ÿå“åº”"
                }[x],
                help="é€‰æ‹©ç”¨äºåˆ†æçš„Google Geminiæ¨¡å‹"
            )
        
        # é«˜çº§è®¾ç½®
        with st.expander("âš™ï¸ é«˜çº§è®¾ç½®"):
            enable_memory = st.checkbox(
                "å¯ç”¨è®°å¿†åŠŸèƒ½",
                value=False,
                help="å¯ç”¨æ™ºèƒ½ä½“è®°å¿†åŠŸèƒ½ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰"
            )
            
            enable_debug = st.checkbox(
                "è°ƒè¯•æ¨¡å¼",
                value=False,
                help="å¯ç”¨è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯è¾“å‡º"
            )
            
            max_tokens = st.slider(
                "æœ€å¤§è¾“å‡ºé•¿åº¦",
                min_value=1000,
                max_value=8000,
                value=4000,
                step=500,
                help="AIæ¨¡å‹çš„æœ€å¤§è¾“å‡ºtokenæ•°é‡"
            )
        
        st.markdown("---")

        # ç³»ç»Ÿé…ç½®
        st.markdown("**ğŸ”§ ç³»ç»Ÿé…ç½®**")

        # APIå¯†é’¥çŠ¶æ€
        st.markdown("**ğŸ”‘ APIå¯†é’¥çŠ¶æ€**")

        def validate_api_key(key, expected_format):
            """éªŒè¯APIå¯†é’¥æ ¼å¼"""
            if not key:
                return "æœªé…ç½®", "error"

            if expected_format == "dashscope" and key.startswith("sk-") and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "deepseek" and key.startswith("sk-") and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "finnhub" and len(key) >= 20:
                return f"{key[:8]}...", "success"
            elif expected_format == "tushare" and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "google" and key.startswith("AIza") and len(key) >= 32:
                return f"{key[:8]}...", "success"
            elif expected_format == "openai" and key.startswith("sk-") and len(key) >= 40:
                return f"{key[:8]}...", "success"
            elif expected_format == "anthropic" and key.startswith("sk-") and len(key) >= 40:
                return f"{key[:8]}...", "success"
            elif expected_format == "reddit" and len(key) >= 10:
                return f"{key[:8]}...", "success"
            else:
                return f"{key[:8]}... (æ ¼å¼å¼‚å¸¸)", "warning"

        # AIæ¨¡å‹APIå¯†é’¥çŠ¶æ€
        st.markdown("*AIæ¨¡å‹é…ç½®:*")

        # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªAIæ¨¡å‹APIå¯†é’¥
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        dashscope_key = os.getenv("DASHSCOPE_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        google_key = os.getenv("GOOGLE_API_KEY")
        
        ai_models_configured = []
        
        # DeepSeek V3
        status, level = validate_api_key(deepseek_key, "deepseek")
        if level == "success":
            st.success(f"âœ… DeepSeek V3: {status}")
            ai_models_configured.append("DeepSeek")
        elif level == "warning":
            st.warning(f"âš ï¸ DeepSeek V3: {status}")
        else:
            st.info("ğŸ’¡ DeepSeek V3: æœªé…ç½®")

        # é˜¿é‡Œç™¾ç‚¼
        status, level = validate_api_key(dashscope_key, "dashscope")
        if level == "success":
            st.success(f"âœ… é˜¿é‡Œç™¾ç‚¼: {status}")
            ai_models_configured.append("é˜¿é‡Œç™¾ç‚¼")
        elif level == "warning":
            st.warning(f"âš ï¸ é˜¿é‡Œç™¾ç‚¼: {status}")
        else:
            st.info("ğŸ’¡ é˜¿é‡Œç™¾ç‚¼: æœªé…ç½®")

        # OpenAI
        status, level = validate_api_key(openai_key, "openai")
        if level == "success":
            st.success(f"âœ… OpenAI: {status}")
            ai_models_configured.append("OpenAI")
        elif level == "warning":
            st.warning(f"âš ï¸ OpenAI: {status}")
        else:
            st.info("ğŸ’¡ OpenAI: æœªé…ç½®")

        # Google AI
        status, level = validate_api_key(google_key, "google")
        if level == "success":
            st.success(f"âœ… Google AI: {status}")
            ai_models_configured.append("Google AI")
        elif level == "warning":
            st.warning(f"âš ï¸ Google AI: {status}")
        else:
            st.info("ğŸ’¡ Google AI: æœªé…ç½®")

        # æ˜¾ç¤ºAIæ¨¡å‹é…ç½®çŠ¶æ€
        if ai_models_configured:
            st.success(f"ğŸ¯ å·²é…ç½®AIæ¨¡å‹: {', '.join(ai_models_configured)}")
        else:
            st.error("âŒ è¯·è‡³å°‘é…ç½®ä¸€ä¸ªAIæ¨¡å‹APIå¯†é’¥")

        # å¿…éœ€çš„æ•°æ®APIå¯†é’¥
        st.markdown("*æ•°æ®æºé…ç½®:*")

        # FinnHub
        finnhub_key = os.getenv("FINNHUB_API_KEY")
        status, level = validate_api_key(finnhub_key, "finnhub")
        if level == "success":
            st.success(f"âœ… FinnHub: {status}")
        elif level == "warning":
            st.warning(f"âš ï¸ FinnHub: {status}")
        else:
            st.error("âŒ FinnHub: æœªé…ç½®")

        # å¯é€‰çš„APIå¯†é’¥
        st.markdown("*å¯é€‰é…ç½®:*")

        # DeepSeek
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        status, level = validate_api_key(deepseek_key, "deepseek")
        if level == "success":
            st.success(f"âœ… DeepSeek: {status}")
        elif level == "warning":
            st.warning(f"âš ï¸ DeepSeek: {status}")
        else:
            st.info("â„¹ï¸ DeepSeek: æœªé…ç½®")

        # Tushare
        tushare_key = os.getenv("TUSHARE_TOKEN")
        status, level = validate_api_key(tushare_key, "tushare")
        if level == "success":
            st.success(f"âœ… Tushare: {status}")
        elif level == "warning":
            st.warning(f"âš ï¸ Tushare: {status}")
        else:
            st.info("â„¹ï¸ Tushare: æœªé…ç½®")

        # Google AI
        google_key = os.getenv("GOOGLE_API_KEY")
        status, level = validate_api_key(google_key, "google")
        if level == "success":
            st.success(f"âœ… Google AI: {status}")
        elif level == "warning":
            st.warning(f"âš ï¸ Google AI: {status}")
        else:
            st.info("â„¹ï¸ Google AI: æœªé…ç½®")

        # OpenAI (å¦‚æœé…ç½®äº†ä¸”ä¸æ˜¯é»˜è®¤å€¼)
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key and openai_key != "your_openai_api_key_here":
            status, level = validate_api_key(openai_key, "openai")
            if level == "success":
                st.success(f"âœ… OpenAI: {status}")
            elif level == "warning":
                st.warning(f"âš ï¸ OpenAI: {status}")

        # Anthropic (å¦‚æœé…ç½®äº†ä¸”ä¸æ˜¯é»˜è®¤å€¼)
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key and anthropic_key != "your_anthropic_api_key_here":
            status, level = validate_api_key(anthropic_key, "anthropic")
            if level == "success":
                st.success(f"âœ… Anthropic: {status}")
            elif level == "warning":
                st.warning(f"âš ï¸ Anthropic: {status}")

        st.markdown("---")

        # ç³»ç»Ÿä¿¡æ¯
        st.markdown("**â„¹ï¸ ç³»ç»Ÿä¿¡æ¯**")
        
        st.info(f"""
        **ç‰ˆæœ¬**: 1.0.0
        **æ¡†æ¶**: Streamlit + LangGraph
        **AIæ¨¡å‹**: {llm_provider.upper()} - {llm_model}
        **æ•°æ®æº**: Tushare + FinnHub API
        """)
        
        # å¸®åŠ©é“¾æ¥
        st.markdown("**ğŸ“š å¸®åŠ©èµ„æº**")
        
        st.markdown("""
        - [ğŸ“– ä½¿ç”¨æ–‡æ¡£](https://github.com/TauricResearch/TradingAgents)
        - [ğŸ› é—®é¢˜åé¦ˆ](https://github.com/TauricResearch/TradingAgents/issues)
        - [ğŸ’¬ è®¨è®ºç¤¾åŒº](https://github.com/TauricResearch/TradingAgents/discussions)
        - [ğŸ”§ APIå¯†é’¥é…ç½®](../docs/security/api_keys_security.md)
        """)
    
    return {
        'llm_provider': llm_provider,
        'llm_model': llm_model,
        'enable_memory': enable_memory,
        'enable_debug': enable_debug,
        'max_tokens': max_tokens
    }
